{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6956346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ../meta_env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a8dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /projects/academic/courses/cse676smr23/asugam/meta_learning_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f452b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp38-cp38-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 619.9 MB 42 kB/s s eta 0:00:01  |                                | 102 kB 5.8 MB/s eta 0:01:48     |▎                               | 5.0 MB 5.8 MB/s eta 0:01:47     |▎                               | 5.6 MB 5.8 MB/s eta 0:01:47     |▉                               | 15.4 MB 8.4 MB/s eta 0:01:12     |█                               | 21.5 MB 10.9 MB/s eta 0:00:55     |█▍                              | 25.9 MB 10.9 MB/s eta 0:00:55     |█▉                              | 35.8 MB 8.5 MB/s eta 0:01:10     |██                              | 39.7 MB 8.5 MB/s eta 0:01:09     |██▍                             | 47.0 MB 11.6 MB/s eta 0:00:50     |██▊                             | 53.1 MB 11.6 MB/s eta 0:00:49     |███▊                            | 71.3 MB 8.4 MB/s eta 0:01:06     |███▊                            | 71.9 MB 8.4 MB/s eta 0:01:06     |███▊                            | 72.5 MB 8.4 MB/s eta 0:01:06     |███▉                            | 73.6 MB 8.4 MB/s eta 0:01:05     |████▍                           | 85.6 MB 8.3 MB/s eta 0:01:05     |████▋                           | 90.0 MB 8.3 MB/s eta 0:01:04     |████▊                           | 91.1 MB 8.3 MB/s eta 0:01:04     |████▉                           | 92.8 MB 8.3 MB/s eta 0:01:04     |█████▋                          | 108.3 MB 11.0 MB/s eta 0:00:47     |██████                          | 114.7 MB 11.0 MB/s eta 0:00:46     |██████▏                         | 119.0 MB 8.5 MB/s eta 0:00:59     |██████▎                         | 121.8 MB 8.5 MB/s eta 0:00:59     |██████▌                         | 126.3 MB 8.5 MB/s eta 0:00:58     |██████▋                         | 128.0 MB 16.2 MB/s eta 0:00:31     |███████                         | 137.4 MB 16.2 MB/s eta 0:00:30     |███████▏                        | 137.9 MB 16.2 MB/s eta 0:00:30     |███████▏                        | 138.5 MB 11.1 MB/s eta 0:00:44     |███████▊                        | 148.9 MB 8.3 MB/s eta 0:00:58     |████████▍                       | 162.2 MB 7.0 MB/s eta 0:01:06     |████████▌                       | 165.6 MB 7.0 MB/s eta 0:01:06     |████████▋                       | 167.7 MB 7.0 MB/s eta 0:01:05     |████████▊                       | 169.4 MB 7.0 MB/s eta 0:01:05     |█████████                       | 172.7 MB 10.1 MB/s eta 0:00:45     |█████████▎                      | 179.5 MB 10.1 MB/s eta 0:00:44     |█████████▋                      | 185.6 MB 9.1 MB/s eta 0:00:48     |██████████▏                     | 196.2 MB 10.7 MB/s eta 0:00:40     |███████████▊                    | 227.4 MB 13.5 MB/s eta 0:00:30     |███████████▉                    | 230.3 MB 13.5 MB/s eta 0:00:29     |████████████                    | 230.9 MB 13.5 MB/s eta 0:00:29     |████████████▌                   | 242.1 MB 7.4 MB/s eta 0:00:52     |█████████████▌                  | 261.4 MB 7.2 MB/s eta 0:00:50     |██████████████                  | 271.2 MB 7.5 MB/s eta 0:00:47     |██████████████▍                 | 279.3 MB 13.2 MB/s eta 0:00:26     |██████████████▌                 | 279.8 MB 13.2 MB/s eta 0:00:26     |███████████████▏                | 293.3 MB 10.9 MB/s eta 0:00:30     |███████████████▎                | 296.2 MB 10.9 MB/s eta 0:00:30     |████████████████                | 311.3 MB 11.2 MB/s eta 0:00:28     |████████████████▎               | 315.4 MB 10.4 MB/s eta 0:00:30     |█████████████████▎              | 335.1 MB 14.2 MB/s eta 0:00:21     |█████████████████▍              | 336.2 MB 14.2 MB/s eta 0:00:20     |█████████████████▍              | 337.4 MB 14.2 MB/s eta 0:00:20     |█████████████████▌              | 339.6 MB 14.2 MB/s eta 0:00:20     |█████████████████▋              | 340.2 MB 14.2 MB/s eta 0:00:20     |█████████████████▉              | 344.7 MB 10.8 MB/s eta 0:00:26     |█████████████████▉              | 346.5 MB 10.8 MB/s eta 0:00:26     |██████████████████              | 348.3 MB 10.8 MB/s eta 0:00:26     |██████████████████▎             | 353.4 MB 10.8 MB/s eta 0:00:25     |██████████████████▎             | 354.0 MB 10.8 MB/s eta 0:00:25     |██████████████████▌             | 359.3 MB 11.4 MB/s eta 0:00:23     |███████████████████             | 366.3 MB 11.4 MB/s eta 0:00:23     |███████████████████             | 366.8 MB 11.4 MB/s eta 0:00:23     |███████████████████▏            | 371.9 MB 7.8 MB/s eta 0:00:32     |███████████████████▌            | 377.7 MB 7.8 MB/s eta 0:00:31     |███████████████████▉            | 385.2 MB 8.8 MB/s eta 0:00:27     |████████████████████            | 388.6 MB 8.8 MB/s eta 0:00:27     |████████████████████▏           | 391.5 MB 10.8 MB/s eta 0:00:22     |████████████████████▌           | 396.1 MB 10.8 MB/s eta 0:00:21     |████████████████████▋           | 399.1 MB 10.8 MB/s eta 0:00:21     |█████████████████████▏          | 409.8 MB 7.5 MB/s eta 0:00:29     |█████████████████████▍          | 414.5 MB 11.2 MB/s eta 0:00:19     |█████████████████████▋          | 418.1 MB 11.2 MB/s eta 0:00:19     |██████████████████████          | 427.3 MB 11.1 MB/s eta 0:00:18     |██████████████████████▏         | 429.0 MB 11.1 MB/s eta 0:00:18     |██████████████████████▎         | 430.8 MB 11.1 MB/s eta 0:00:17     |██████████████████████▌         | 436.7 MB 14.4 MB/s eta 0:00:13     |██████████████████████▋         | 438.5 MB 14.4 MB/s eta 0:00:13     |███████████████████████         | 444.3 MB 14.4 MB/s eta 0:00:13     |███████████████████████         | 444.9 MB 14.4 MB/s eta 0:00:13     |███████████████████████         | 446.1 MB 11.1 MB/s eta 0:00:16     |███████████████████████▏        | 449.0 MB 11.1 MB/s eta 0:00:16     |███████████████████████▍        | 453.7 MB 11.1 MB/s eta 0:00:15     |███████████████████████▋        | 458.4 MB 14.4 MB/s eta 0:00:12     |███████████████████████▉        | 462.4 MB 14.4 MB/s eta 0:00:11     |████████████████████████▏       | 467.7 MB 8.4 MB/s eta 0:00:19     |████████████████████████▉       | 480.4 MB 10.8 MB/s eta 0:00:13     |████████████████████████▉       | 480.9 MB 10.8 MB/s eta 0:00:13     |█████████████████████████       | 483.3 MB 10.8 MB/s eta 0:00:13     |█████████████████████████       | 484.4 MB 10.8 MB/s eta 0:00:13     |█████████████████████████▏      | 487.9 MB 10.8 MB/s eta 0:00:13     |█████████████████████████▍      | 491.4 MB 11.2 MB/s eta 0:00:12     |██████████████████████████▋     | 515.6 MB 10.7 MB/s eta 0:00:10     |██████████████████████████▉     | 519.6 MB 10.7 MB/s eta 0:00:10     |███████████████████████████▊    | 536.8 MB 11.1 MB/s eta 0:00:08     |███████████████████████████▊    | 537.4 MB 11.1 MB/s eta 0:00:08     |███████████████████████████▉    | 539.2 MB 11.1 MB/s eta 0:00:08     |████████████████████████████    | 542.2 MB 11.1 MB/s eta 0:00:08     |████████████████████████████    | 542.7 MB 11.1 MB/s eta 0:00:07     |████████████████████████████▌   | 552.6 MB 7.3 MB/s eta 0:00:10     |████████████████████████████▊   | 556.2 MB 7.3 MB/s eta 0:00:09     |████████████████████████████▊   | 556.7 MB 7.3 MB/s eta 0:00:09     |█████████████████████████████▏  | 564.2 MB 10.9 MB/s eta 0:00:06     |█████████████████████████████▏  | 564.8 MB 10.9 MB/s eta 0:00:06     |█████████████████████████████▏  | 565.9 MB 10.9 MB/s eta 0:00:05     |█████████████████████████████▍  | 568.8 MB 11.0 MB/s eta 0:00:05     |█████████████████████████████▍  | 569.4 MB 11.0 MB/s eta 0:00:05     |█████████████████████████████▌  | 570.5 MB 11.0 MB/s eta 0:00:05     |█████████████████████████████▋  | 574.0 MB 11.0 MB/s eta 0:00:05     |██████████████████████████████▌ | 591.6 MB 10.2 MB/s eta 0:00:03     |██████████████████████████████▋ | 592.2 MB 10.2 MB/s eta 0:00:03     |███████████████████████████████ | 601.6 MB 10.2 MB/s eta 0:00:02     |███████████████████████████████▏| 602.7 MB 12.6 MB/s eta 0:00:02     |███████████████████████████████▍| 607.9 MB 12.6 MB/s eta 0:00:01     |███████████████████████████████▋| 611.4 MB 12.6 MB/s eta 0:00:01     |███████████████████████████████▉| 616.0 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp38-cp38-manylinux1_x86_64.whl (33.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 33.8 MB 11.1 MB/s eta 0:00:01    |████████▍                       | 8.9 MB 45.4 MB/s eta 0:00:01     |█████████████████               | 18.0 MB 11.0 MB/s eta 0:00:02     |███████████████████▎            | 20.3 MB 11.0 MB/s eta 0:00:02     |████████████████████████▉       | 26.2 MB 8.8 MB/s eta 0:00:01     |███████████████████████████▌    | 29.1 MB 8.8 MB/s eta 0:00:01     |█████████████████████████████▊  | 31.4 MB 8.8 MB/s eta 0:00:01     |██████████████████████████████▎ | 32.0 MB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (3.3.4)\n",
      "Requirement already satisfied: tqdm in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (4.59.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from matplotlib) (8.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 2.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 63.2 MB 11.1 MB/s eta 0:00:01    |██▌                             | 5.0 MB 45.7 MB/s eta 0:00:02     |████████████████▍               | 32.3 MB 8.0 MB/s eta 0:00:04     |███████████████████▌            | 38.6 MB 10.9 MB/s eta 0:00:03     |███████████████████████         | 45.6 MB 11.0 MB/s eta 0:00:02     |███████████████████████▊        | 46.8 MB 11.0 MB/s eta 0:00:02     |████████████████████████▎       | 48.0 MB 11.0 MB/s eta 0:00:02     |████████████████████████████▏   | 55.6 MB 11.0 MB/s eta 0:00:01     |█████████████████████████████▉  | 59.0 MB 11.1 MB/s eta 0:00:01     |██████████████████████████████▏ | 59.6 MB 11.1 MB/s eta 0:00:01     |███████████████████████████████▍| 62.0 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 177.1 MB 232 kB/s eta 0:00:011   |▉                               | 4.8 MB 44.4 MB/s eta 0:00:04     |█                               | 5.4 MB 44.4 MB/s eta 0:00:04     |█▍                              | 7.7 MB 44.4 MB/s eta 0:00:04     |██▉                             | 15.8 MB 10.6 MB/s eta 0:00:16     |███                             | 16.4 MB 10.6 MB/s eta 0:00:16     |███▋                            | 19.9 MB 10.6 MB/s eta 0:00:15     |████▌                           | 25.1 MB 8.7 MB/s eta 0:00:18     |█████▌                          | 30.3 MB 8.7 MB/s eta 0:00:17     |█████▊                          | 31.5 MB 8.7 MB/s eta 0:00:17     |█████▉                          | 32.0 MB 8.7 MB/s eta 0:00:17     |██████                          | 32.6 MB 8.7 MB/s eta 0:00:17     |██████                          | 33.7 MB 11.1 MB/s eta 0:00:13     |██████▌                         | 36.0 MB 11.1 MB/s eta 0:00:13     |███████▋                        | 42.4 MB 11.1 MB/s eta 0:00:13     |████████▊                       | 48.1 MB 11.0 MB/s eta 0:00:12     |██████████▊                     | 59.1 MB 11.1 MB/s eta 0:00:11     |███████████                     | 61.4 MB 11.1 MB/s eta 0:00:11     |███████████▉                    | 65.6 MB 11.1 MB/s eta 0:00:11     |████████████▍                   | 68.4 MB 7.5 MB/s eta 0:00:15     |█████████████▍                  | 74.3 MB 7.5 MB/s eta 0:00:14     |█████████████▌                  | 74.9 MB 7.5 MB/s eta 0:00:14     |█████████████▋                  | 75.4 MB 7.5 MB/s eta 0:00:14     |██████████████                  | 77.2 MB 7.5 MB/s eta 0:00:14     |██████████████▉                 | 82.4 MB 10.5 MB/s eta 0:00:09     |████████████████▏               | 89.4 MB 10.6 MB/s eta 0:00:09     |████████████████▊               | 92.8 MB 10.6 MB/s eta 0:00:08     |█████████████████▍              | 96.4 MB 10.6 MB/s eta 0:00:08     |█████████████████▋              | 97.5 MB 10.6 MB/s eta 0:00:08     |██████████████████              | 99.9 MB 10.6 MB/s eta 0:00:08     |████████████████████            | 111.0 MB 13.6 MB/s eta 0:00:05     |████████████████████▊           | 114.4 MB 10.9 MB/s eta 0:00:06     |█████████████████████▎          | 117.8 MB 10.9 MB/s eta 0:00:06     |█████████████████████▋          | 119.5 MB 10.9 MB/s eta 0:00:06     |█████████████████████▊          | 120.1 MB 10.9 MB/s eta 0:00:06     |██████████████████████          | 121.3 MB 10.9 MB/s eta 0:00:06     |███████████████████████         | 127.7 MB 8.2 MB/s eta 0:00:07     |████████████████████████        | 132.9 MB 8.2 MB/s eta 0:00:06     |██████████████████████████▎     | 145.5 MB 13.3 MB/s eta 0:00:03     |███████████████████████████▍    | 151.3 MB 13.3 MB/s eta 0:00:02     |███████████████████████████▌    | 151.9 MB 13.3 MB/s eta 0:00:02     |████████████████████████████▊   | 158.8 MB 11.6 MB/s eta 0:00:02     |████████████████████████████▉   | 159.3 MB 11.6 MB/s eta 0:00:02     |█████████████████████████████▍  | 162.8 MB 11.6 MB/s eta 0:00:02     |█████████████████████████████▌  | 163.4 MB 11.6 MB/s eta 0:00:02     |█████████████████████████████▊  | 164.5 MB 11.6 MB/s eta 0:00:02     |██████████████████████████████  | 165.7 MB 11.6 MB/s eta 0:00:01     |██████████████████████████████▋ | 169.7 MB 8.2 MB/s eta 0:00:01     |██████████████████████████████▉ | 170.9 MB 8.2 MB/s eta 0:00:01     |████████████████████████████████| 177.1 MB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 168.4 MB 10.2 MB/s eta 0:00:01   |▎                               | 1.3 MB 64.9 MB/s eta 0:00:03     |█▏                              | 5.9 MB 64.9 MB/s eta 0:00:03     |█▎                              | 6.5 MB 64.9 MB/s eta 0:00:03     |███▏                            | 16.8 MB 12.7 MB/s eta 0:00:12     |███▉                            | 20.2 MB 12.7 MB/s eta 0:00:12     |█████▎                          | 27.7 MB 9.2 MB/s eta 0:00:16     |████████                        | 42.1 MB 14.5 MB/s eta 0:00:09     |████████▏                       | 42.7 MB 14.5 MB/s eta 0:00:09     |████████▎                       | 43.3 MB 14.5 MB/s eta 0:00:09     |████████▊                       | 46.2 MB 11.1 MB/s eta 0:00:11     |█████████                       | 47.4 MB 11.1 MB/s eta 0:00:11     |██████████▍                     | 54.9 MB 11.1 MB/s eta 0:00:11     |███████████▎                    | 59.6 MB 11.1 MB/s eta 0:00:10     |████████████                    | 63.1 MB 11.1 MB/s eta 0:00:10     |████████████▌                   | 65.9 MB 11.1 MB/s eta 0:00:10     |█████████████                   | 68.3 MB 11.0 MB/s eta 0:00:10     |█████████████▎                  | 69.9 MB 11.0 MB/s eta 0:00:09     |█████████████▍                  | 70.5 MB 11.0 MB/s eta 0:00:09     |█████████████▊                  | 72.3 MB 11.0 MB/s eta 0:00:09     |█████████████▉                  | 72.8 MB 11.0 MB/s eta 0:00:09     |██████████████                  | 74.0 MB 11.0 MB/s eta 0:00:09     |██████████████▊                 | 77.4 MB 11.0 MB/s eta 0:00:09     |███████████████▍                | 80.9 MB 8.5 MB/s eta 0:00:11     |███████████████▊                | 82.6 MB 8.5 MB/s eta 0:00:11     |█████████████████               | 89.6 MB 11.2 MB/s eta 0:00:08     |█████████████████▍              | 91.3 MB 11.2 MB/s eta 0:00:07     |██████████████████              | 94.8 MB 11.2 MB/s eta 0:00:07     |███████████████████             | 99.5 MB 11.2 MB/s eta 0:00:07     |███████████████████▊            | 103.6 MB 9.3 MB/s eta 0:00:07     |████████████████████            | 104.7 MB 9.3 MB/s eta 0:00:07     |████████████████████▉           | 109.4 MB 9.3 MB/s eta 0:00:07     |████████████████████▉           | 109.9 MB 9.3 MB/s eta 0:00:07     |█████████████████████▍          | 112.8 MB 13.8 MB/s eta 0:00:05     |██████████████████████          | 115.7 MB 13.8 MB/s eta 0:00:04     |██████████████████████▎         | 117.5 MB 13.8 MB/s eta 0:00:04     |██████████████████████▍         | 118.0 MB 13.8 MB/s eta 0:00:04     |██████████████████████▌         | 118.7 MB 13.8 MB/s eta 0:00:04     |███████████████████████▌        | 123.9 MB 17.3 MB/s eta 0:00:03     |███████████████████████▊        | 125.1 MB 17.3 MB/s eta 0:00:03     |████████████████████████▏       | 127.4 MB 17.3 MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 557.1 MB 31 kB/s s eta 0:00:01   |▎                               | 4.8 MB 47.7 MB/s eta 0:00:12     |▋                               | 10.6 MB 47.7 MB/s eta 0:00:12     |█                               | 15.8 MB 9.5 MB/s eta 0:00:58\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: networkx in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from torch) (2.5)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 317.1 MB 13.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.8 MB 10.9 MB/s eta 0:00:01    |███                             | 1.1 MB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 54.6 MB 618 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 173.2 MB 10.7 MB/s eta 0:00:01   |                                | 204 kB 44.3 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 102.6 MB 12.0 MB/s eta 0:00:01   |▎                               | 788 kB 41.9 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: sympy in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from torch) (1.8)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[K     |████████████████████████████████| 849 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 9.5 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: filelock in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from torch) (3.0.12)\n",
      "Requirement already satisfied: wheel in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.36.2)\n",
      "Requirement already satisfied: setuptools in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (52.0.0.post20210125)\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[K     |████████████████████████████████| 153 kB 43.6 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Downloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.0 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from jinja2->torch) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from networkx->torch) (5.0.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from requests->torchvision) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from requests->torchvision) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from requests->torchvision) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: mpmath>=0.19 in /util/common/python/ondemand-jupyter/py38/anaconda-2021.05/lib/python3.8/site-packages (from sympy->torch) (1.2.1)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93582 sha256=9456ae2962f0aaf2cf5d7c20457514be5f7eb8928241fdd45d9f725375ee3830\n",
      "  Stored in directory: /user/asugam/.cache/pip/wheels/05/ab/f1/0102fea49a41c753f0e79a1a4012417d5d7ef0f93224694472\n",
      "Successfully built lit\n",
      "Installing collected packages: nvidia-cublas-cu11, lit, cmake, triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-cusolver-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, torch, torchvision\n",
      "Successfully installed cmake-3.26.4 lit-16.0.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchvision-0.15.2 triton-2.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029f7a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import LinearLR\n",
    "\n",
    "\n",
    "\n",
    "class MyTransform:\n",
    "    def __init__(self, image_size, horizontal_flip=False, rotation_angle=None):\n",
    "        transform_list = [\n",
    "            #transforms.RandomResizedCrop(image_size),\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "        if horizontal_flip:\n",
    "            transform_list.append(transforms.RandomHorizontalFlip())\n",
    "        if rotation_angle is not None:\n",
    "            transform_list.append(transforms.RandomRotation(degrees=rotation_angle))\n",
    "\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.transform(img)\n",
    "\n",
    "# Path to the ImageNet dataset directory\n",
    "#data_dir = './ImageNet'\n",
    "\n",
    "# Parameters for image augmentation\n",
    "image_size = 224\n",
    "horizontal_flip = False\n",
    "rotation_angle = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8284242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# define configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.n_way = 3  # N-way\n",
    "        self.k_spt = 5  # K-shot\n",
    "        self.k_query = self.k_qry =10  # K-query\n",
    "        self.img_size = (224, 224)  # size of images\n",
    "        self.batch_size = 1  # batch size\n",
    "        self.inner_update_lr = 0.01  # inner update learning rate\n",
    "        self.meta_lr = 1e-3  # meta learning rate\n",
    "        self.meta_batch_size = 10  # size of meta batch\n",
    "        self.max_epoch = 10  # number of epochs\n",
    "        self.num_tasks = 10  # number of tasks\n",
    "\n",
    "config = Config()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e78edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "'''\n",
    "try:\n",
    "    !rm -r /projects/academic/courses/cse676smr23/asugam/meta_learning_project/data/meta_tasks_symlink\n",
    "except:\n",
    "    print(\"nothing to delete\")\n",
    "finally:\n",
    "    !mkdir /projects/academic/courses/cse676smr23/asugam/meta_learning_project/data/meta_tasks_symlink\n",
    "\n",
    "try:\n",
    "    !rm -r /projects/academic/courses/cse676smr23/asugam/meta_learning_project/data/meta_test_tasks_symlink\n",
    "except:\n",
    "    print(\"nothing to delete\")\n",
    "finally:\n",
    "    !mkdir /projects/academic/courses/cse676smr23/asugam/meta_learning_project/data/meta_test_tasks_symlink\n",
    "'''\n",
    "class MetaLearningDataset(Dataset):\n",
    "    def __init__(self, root_dir, num_tasks, num_classes, support_set_size, query_set_size,transform = None, is_test_loader=False,seed = 0):\n",
    "        self.num_tasks = num_tasks\n",
    "        self.num_classes = num_classes\n",
    "        self.support_set_size = support_set_size\n",
    "        self.query_set_size = query_set_size\n",
    "        self.seed = seed\n",
    "        self.transform = transform\n",
    "        self.is_test_loader = is_test_loader\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        if self.is_test_loader:\n",
    "            self.task_dir = '/projects/academic/courses/cse676smr23/asugam/meta_learning_project/data/meta_test_tasks_symlink'\n",
    "        else:\n",
    "            self.task_dir = '/projects/academic/courses/cse676smr23/asugam/meta_learning_project/data/meta_tasks_symlink'\n",
    "        self.class_names = os.listdir(self.root_dir)\n",
    "        \n",
    "        self.task_seeds = np.arange(seed, seed + num_tasks)\n",
    "        np.random.shuffle(self.task_seeds)\n",
    "\n",
    "        # Create task directories\n",
    "        for task_seed in self.task_seeds:\n",
    "            task_seed = int(task_seed)\n",
    "            task_dir_path = os.path.join(self.task_dir, f'task_{task_seed}')\n",
    "            if os.path.exists(task_dir_path) and os.path.isdir(task_dir_path):\n",
    "                continue\n",
    "            \n",
    "            random.seed(task_seed)\n",
    "            np.random.seed(task_seed)\n",
    "\n",
    "            task_classes = random.sample(self.class_names, self.num_classes)\n",
    "            print(f'### {task_classes}')\n",
    "            # Create symbolic links to the chosen class directories in the task directory\n",
    "            for class_name in task_classes:\n",
    "                class_dir = os.path.join(self.root_dir, class_name)\n",
    "                class_images = os.listdir(class_dir)\n",
    "                task_images = random.sample(class_images, min(self.support_set_size + self.query_set_size, len(class_images)))\n",
    "                for img_name in task_images:\n",
    "                    src = os.path.join(class_dir, img_name)\n",
    "                    dst_dir = os.path.join(self.task_dir, f'task_{task_seed}', class_name)\n",
    "                    os.makedirs(dst_dir, exist_ok=True)\n",
    "                    dst = os.path.join(dst_dir, img_name)\n",
    "                    os.symlink(src, dst)\n",
    "        print('created symlinks for each task')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        task_seed = int(self.task_seeds[index%self.num_tasks])\n",
    "        \n",
    "        task_folder = os.path.join(self.task_dir, f'task_{task_seed}')\n",
    "        \n",
    "        # Load the task directory using ImageFolder\n",
    "        \n",
    "        image_dataset = ImageFolder(task_folder, transform=self.transform)\n",
    "\n",
    "        # Randomly select the support and query set\n",
    "        random.seed(task_seed)\n",
    "        np.random.seed(task_seed)\n",
    "\n",
    "        support_images = []\n",
    "        support_labels = []\n",
    "        query_images = []\n",
    "        query_labels = []\n",
    "\n",
    "        for class_idx in range(self.num_classes):\n",
    "            # Get all images of this class\n",
    "            class_images = [(img, target) for img, target in image_dataset if target == class_idx]\n",
    "            \n",
    "            # Ensure there are enough images for support and query set\n",
    "            if not self.is_test_loader:\n",
    "                assert len(class_images) >= self.support_set_size + self.query_set_size\n",
    "\n",
    "            # Randomly select images for support set and query set\n",
    "            indices = list(range(len(class_images)))\n",
    "            random.shuffle(indices)\n",
    "\n",
    "            # Select support_set_size images for support set\n",
    "            for i in indices[:self.support_set_size]:\n",
    "                img, label = class_images[i]\n",
    "                support_images.append(img)\n",
    "                support_labels.append(torch.tensor(class_idx))\n",
    "\n",
    "            for i in indices[self.support_set_size:self.support_set_size + self.query_set_size]:\n",
    "                img, label = class_images[i]\n",
    "                query_images.append(img)\n",
    "                query_labels.append(torch.tensor(class_idx))\n",
    "                \n",
    "        # Shuffle support set and query set\n",
    "        support_set = list(zip(support_images, support_labels))\n",
    "        random.shuffle(support_set)\n",
    "        support_images, support_labels = zip(*support_set)\n",
    "\n",
    "        query_set = list(zip(query_images, query_labels))\n",
    "        random.shuffle(query_set)\n",
    "        query_images, query_labels = zip(*query_set)\n",
    "\n",
    "        # Convert lists of images and labels to tensors\n",
    "        support_set_X_batch = torch.stack(support_images)\n",
    "        support_set_y_batch = torch.stack(support_labels)\n",
    "        query_set_X_batch = torch.stack(query_images)\n",
    "        query_set_y_batch = torch.stack(query_labels)\n",
    "\n",
    "        return support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "836bca61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = '/projects/academic/courses/cse676smr23/asugam/meta_learning_project/data/train_tasks' \n",
    "test_dataset_path = '/projects/academic/courses/cse676smr23/asugam/meta_learning_project/data/big_final_category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65452b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created symlinks for each task\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_meta_learning_loader = MetaLearningDataset(root_dir = test_dataset_path , is_test_loader = True, num_tasks = 1, num_classes = 3, support_set_size = 5, query_set_size = 500,transform =MyTransform(image_size, horizontal_flip, rotation_angle) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f10d18e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created symlinks for each task\n"
     ]
    }
   ],
   "source": [
    "\n",
    "meta_learning_loader = MetaLearningDataset(root_dir = train_dataset_path , num_tasks = 100, num_classes = 3, support_set_size = 5, query_set_size = 10,transform =MyTransform(image_size, horizontal_flip, rotation_angle) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a64e2892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = meta_learning_loader[7]\n",
    "val[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cefb2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class MyPretrainedAlexNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(MyPretrainedAlexNet, self).__init__()\n",
    "        self.alexnet = models.alexnet(pretrained=True)\n",
    "        self.alexnet.classifier[6] = torch.nn.Linear(self.alexnet.classifier[6].in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.alexnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "007cca9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import torchvision.models as models\n",
    "\n",
    "class MyAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=3, use_dropout=False, reg_strength = 0.2):\n",
    "        super(MyAlexNet, self).__init__()\n",
    "\n",
    "        self.reg_strength = reg_strength\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout() if use_dropout else nn.Identity(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout() if use_dropout else nn.Identity(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def l1_regularization_loss(self):\n",
    "        l1_loss = torch.tensor(0.0).to(self.features[0].weight.device)\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.norm(param, p=1)\n",
    "        return self.reg_strength * l1_loss\n",
    "\n",
    "    def l2_regularization_loss(self):\n",
    "        l2_loss = torch.tensor(0.0).to(self.features[0].weight.device)\n",
    "        for param in self.parameters():\n",
    "            l2_loss += torch.norm(param, p=2)\n",
    "        return self.reg_strength * l2_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class MyResNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(MyResNet, self).__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = torch.nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6327560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def confusion_mat(true_labels, predicted_labels):\n",
    "\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Compute the classification accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Create a list of class labels\n",
    "\n",
    "    # Create a heatmap of the confusion matrix using seaborn\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='RdGy')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class MAMLFewShot:\n",
    "    def __init__(self, name, model,dataloaders, test_dataloader=None, tasks=100, n_shot=3, epochs=50, lr_inner=0.01, lr_outer=0.001, num_inner_updates=10, device = 'cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        #print(self.model)\n",
    "        #print(self.device)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.name = name\n",
    "        self.tasks = tasks\n",
    "        self.n_shot = n_shot\n",
    "        self.epochs = epochs\n",
    "        self.lr_inner = lr_inner\n",
    "        self.lr_outer = lr_outer\n",
    "        self.dataloaders = dataloaders\n",
    "        self.num_inner_updates = num_inner_updates\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr_outer)\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "    def inner_loop(self, task_support_set):\n",
    "\n",
    "        model_copy = MyAlexNet()\n",
    "        model_copy = model_copy.to(self.device)\n",
    "        model_copy.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        optimizer = optim.SGD(model_copy.parameters(), lr=self.lr_inner)\n",
    "\n",
    "        # Select subset of data for few-shot learning\n",
    "        X_train, y_train = task_support_set\n",
    "\n",
    "\n",
    "        #X_train = torch.squeeze(X_train)\n",
    "        for t_epoch in range(self.num_inner_updates):\n",
    "            preds = model_copy(X_train)\n",
    "            loss = nn.CrossEntropyLoss()(preds, y_train)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print(\"\\t  iter {}:   task loss: {}\".format(t_epoch+1, loss.item()))\n",
    "\n",
    "        return model_copy\n",
    "\n",
    "\n",
    "    def outer_loop(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        task_losses = []\n",
    "\n",
    "        for task in range(self.tasks):\n",
    "            #print(\"\\nWorking with task: {}\".format(task + 1))\n",
    "            support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch = self.dataloaders[task]\n",
    "\n",
    "            if len(query_set_X_batch.shape) == 5:\n",
    "                support_set_X_batch = support_set_X_batch.squeeze(0)\n",
    "\n",
    "            if len(query_set_X_batch.shape) == 5:\n",
    "                query_set_X_batch = query_set_X_batch.squeeze(0)\n",
    "\n",
    "            support_set_X_batch =support_set_X_batch.to(self.device)\n",
    "            support_set_y_batch = support_set_y_batch.to(self.device)\n",
    "            query_set_X_batch = query_set_X_batch.to(self.device)\n",
    "            query_set_y_batch = query_set_y_batch.to(self.device)\n",
    "            #print(\"shapes support set \", support_set_X_batch.shape, support_set_y_batch.shape, sep = ' : ')\n",
    "            model_copy = self.inner_loop((support_set_X_batch, support_set_y_batch))\n",
    "\n",
    "\n",
    "\n",
    "            preds = model_copy(query_set_X_batch)\n",
    "            loss = nn.CrossEntropyLoss()(preds, query_set_y_batch)\n",
    "            task_losses.append(loss)\n",
    "            for param, param_copy in zip(self.model.parameters(), model_copy.parameters()):\n",
    "                if param_copy.grad is not None:\n",
    "                    param.grad = param_copy.grad.clone()\n",
    "                else:\n",
    "                    param.grad += param_copy.grad\n",
    "\n",
    "        meta_loss = sum(task_losses) / len(task_losses)\n",
    "        meta_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return meta_loss.item()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        print('Start training...')\n",
    "        best_loss = float('inf')\n",
    "        best_model_weights = None\n",
    "        meta_losses = []\n",
    "        try:\n",
    "          for epoch in tqdm(range(self.epochs)):\n",
    "\n",
    "              meta_loss = self.outer_loop()\n",
    "              meta_losses.append(meta_loss)\n",
    "              if epoch % 1 == 0:\n",
    "                  print('Epoch: %d, Meta Loss: ', epoch, meta_loss)\n",
    "              if meta_loss < best_loss:\n",
    "                  best_loss = meta_loss\n",
    "                  best_model_weights = self.model.state_dict()\n",
    "\n",
    "          torch.save(best_model_weights, '{self.name}_best_model.pth')\n",
    "          print('Training finished.')\n",
    "        except KeyboardInterrupt:\n",
    "          torch.save(self.model.state_dict(), '{self.name}_latest_model_interrupted_loss{meta_loss:.3f}.pth')\n",
    "          if best_model_weights:\n",
    "            torch.save(best_model_weights, '{self.name}_best_model.pth')\n",
    "        finally:\n",
    "          plt.plot(meta_losses)\n",
    "          plt.xlabel('Epoch')\n",
    "          plt.ylabel('Loss')\n",
    "          plt.title('Meta Training Loss')\n",
    "          plt.show()\n",
    "\n",
    "    def test(self, test_dataloader, pretrained_model = None):\n",
    "\n",
    "      if pretrained_model:\n",
    "        state_dict = torch.load(pretrained_model)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "      fast_weights = deepcopy(self.model.state_dict())\n",
    "      for task in range(len(test_dataloader)):\n",
    "\n",
    "          support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch = test_dataloader[task]\n",
    "          if len(query_set_X_batch.shape) == 5:\n",
    "              support_set_X_batch = support_set_X_batch.squeeze(0)\n",
    "\n",
    "          if len(query_set_X_batch.shape) == 5:\n",
    "            query_set_X_batch = query_set_X_batch.squeeze(0)\n",
    "\n",
    "          support_set_X_batch =support_set_X_batch.to(self.device)\n",
    "          support_set_y_batch = support_set_y_batch.to(self.device)\n",
    "          query_set_X_batch = query_set_X_batch.to(self.device)\n",
    "          query_set_y_batch = query_set_y_batch.to(self.device)\n",
    "\n",
    "          model_copy = self.inner_loop((support_set_X_batch, support_set_y_batch))\n",
    "\n",
    "          model_copy.eval()\n",
    "          with torch.no_grad():\n",
    "              preds = model_copy(query_set_X_batch)\n",
    "              _, preds = torch.max(preds, dim=1)\n",
    "              accuracy = accuracy_score(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "              print(f\"Accuracy on test task: {accuracy}\")\n",
    "              conf_mat = confusion_matrix(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "              print(f'Confusion Matrix on test task: \\n{conf_mat}')\n",
    "              confusion_mat(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "\n",
    "      self.model.train()  # Set the model back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdaec737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MAMLCloneFewShot:\n",
    "    def __init__(self, name, model,dataloaders, test_dataloader=None, tasks=100, n_shot=3, epochs=50, lr_inner=0.01, lr_outer=0.001, num_inner_updates=10, device = 'cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        #print(self.model)\n",
    "        #print(self.device)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.name = name\n",
    "        self.tasks = tasks\n",
    "        self.n_shot = n_shot\n",
    "        self.epochs = epochs\n",
    "        self.lr_inner = lr_inner\n",
    "        self.lr_outer = lr_outer\n",
    "        self.dataloaders = dataloaders\n",
    "        self.num_inner_updates = num_inner_updates\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr_outer)\n",
    "        self.test_dataloader = test_dataloader\n",
    "    \n",
    "    def compute_loss(model, data, params=None):\n",
    "        # Assuming data is a tuple of (inputs, targets)\n",
    "        inputs, targets = data\n",
    "        # Use the provided model parameters or the current model parameters\n",
    "        if params is None:\n",
    "            outputs = model(inputs)\n",
    "        else:\n",
    "            outputs = model(inputs, params)\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "        return loss\n",
    "\n",
    "    def inner_loop(self, task_support_set):\n",
    "\n",
    "        fast_weights = [w.clone() for w in self.model.parameters()]\n",
    "\n",
    "\n",
    "        for t_epoch in range(self.num_inner_updates):\n",
    "            loss = compute_loss(model, task_support_set, params=fast_weights)\n",
    "            gradients = torch.autograd.grad(loss, fast_weights, create_graph=True)  \n",
    "            # We set create_graph=True to allow for gradient calculations in the next step\n",
    "            # Now we update the fast weights using the calculated gradients (applying the inner-loop learning rate)\n",
    "            fast_weights = [w - inner_lr * g for w, g in zip(fast_weights, gradients)]\n",
    "\n",
    "        return fast_weights\n",
    "\n",
    "\n",
    "    def outer_loop(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        task_losses = []\n",
    "\n",
    "        for task in range(self.tasks):\n",
    "            #print(\"\\nWorking with task: {}\".format(task + 1))\n",
    "            support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch = self.dataloaders[task]\n",
    "\n",
    "            if len(query_set_X_batch.shape) == 5:\n",
    "                support_set_X_batch = support_set_X_batch.squeeze(0)\n",
    "\n",
    "            if len(query_set_X_batch.shape) == 5:\n",
    "                query_set_X_batch = query_set_X_batch.squeeze(0)\n",
    "\n",
    "            support_set_X_batch =support_set_X_batch.to(self.device)\n",
    "            support_set_y_batch = support_set_y_batch.to(self.device)\n",
    "            query_set_X_batch = query_set_X_batch.to(self.device)\n",
    "            query_set_y_batch = query_set_y_batch.to(self.device)\n",
    "            #print(\"shapes support set \", support_set_X_batch.shape, support_set_y_batch.shape, sep = ' : ')\n",
    "            fast_weights = self.inner_loop((support_set_X_batch, support_set_y_batch))\n",
    "\n",
    "            \n",
    "            loss = compute_loss(self.model, tuple(query_set_X_batch,query_set_y_batch), params=fast_weights)\n",
    "            task_losses.append(loss)\n",
    "            # This is where the second order derivatives are calculated. \n",
    "            # We calculate the gradients of the query set loss with respect to the original model parameters\n",
    "            meta_gradients = torch.autograd.grad(loss, self.model.parameters(), retain_graph=True)\n",
    "            for p, g in zip(self.model.parameters(), meta_gradients):  \n",
    "                if p.grad is None:\n",
    "                    p.grad = g\n",
    "                else:\n",
    "                    p.grad += g\n",
    "\n",
    "        meta_loss = sum(task_losses) / len(task_losses)\n",
    "        meta_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return meta_loss.item()\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        print('Start training...')\n",
    "        best_loss = float('inf')\n",
    "        best_model_weights = None\n",
    "        meta_losses = []\n",
    "        try:\n",
    "          for epoch in tqdm(range(self.epochs)):\n",
    "\n",
    "              meta_loss = self.outer_loop()\n",
    "              meta_losses.append(meta_loss)\n",
    "              if epoch % 1 == 0:\n",
    "                  print('Epoch: %d, Meta Loss: ', epoch, meta_loss)\n",
    "              if meta_loss < best_loss:\n",
    "                  best_loss = meta_loss\n",
    "                  best_model_weights = self.model.state_dict()\n",
    "\n",
    "          torch.save(best_model_weights, '{self.name}_best_model.pth')\n",
    "          print('Training finished.')\n",
    "        except KeyboardInterrupt:\n",
    "          torch.save(self.model.state_dict(), '{self.name}_latest_model_interrupted_loss{meta_loss:.3f}.pth')\n",
    "          if best_model_weights:\n",
    "            torch.save(best_model_weights, '{self.name}_best_model.pth')\n",
    "        finally:\n",
    "          plt.plot(meta_losses)\n",
    "          plt.xlabel('Epoch')\n",
    "          plt.ylabel('Loss')\n",
    "          plt.title('Meta Training Loss')\n",
    "          plt.show()\n",
    "\n",
    "    def test(self, test_dataloader, pretrained_model = None):\n",
    "\n",
    "      if pretrained_model:\n",
    "        state_dict = torch.load(pretrained_model)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "      fast_weights = deepcopy(self.model.state_dict())\n",
    "      for task in range(len(test_dataloader)):\n",
    "\n",
    "          support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch = test_dataloader[task]\n",
    "          if len(query_set_X_batch.shape) == 5:\n",
    "              support_set_X_batch = support_set_X_batch.squeeze(0)\n",
    "\n",
    "          if len(query_set_X_batch.shape) == 5:\n",
    "            query_set_X_batch = query_set_X_batch.squeeze(0)\n",
    "\n",
    "          support_set_X_batch =support_set_X_batch.to(self.device)\n",
    "          support_set_y_batch = support_set_y_batch.to(self.device)\n",
    "          query_set_X_batch = query_set_X_batch.to(self.device)\n",
    "          query_set_y_batch = query_set_y_batch.to(self.device)\n",
    "\n",
    "          model_copy = self.inner_loop((support_set_X_batch, support_set_y_batch))\n",
    "\n",
    "          model_copy.eval()\n",
    "          with torch.no_grad():\n",
    "              preds = model_copy(query_set_X_batch)\n",
    "              _, preds = torch.max(preds, dim=1)\n",
    "              accuracy = accuracy_score(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "              print(f\"Accuracy on test task: {accuracy}\")\n",
    "              conf_mat = confusion_matrix(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "              print(f'Confusion Matrix on test task: \\n{conf_mat}')\n",
    "              confusion_mat(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "\n",
    "      self.model.train()  # Set the model back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99eb759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "class Reptile:\n",
    "    def __init__(self, name, model,dataloaders, test_dataloader=None, tasks=100, n_shot=3, epochs=50, inner_lr=0.01, meta_lr=0.001, inner_steps=10, device = 'cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        #print(self.model)\n",
    "        #print(self.device)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.name = name\n",
    "        self.tasks = tasks\n",
    "        self.n_shot = n_shot\n",
    "        self.epochs = epochs\n",
    "        self.dataloaders = dataloaders\n",
    "        #self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr_outer)\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.inner_optimizer = optim.SGD(self.model.parameters(), lr=self.inner_lr)\n",
    "\n",
    "    def inner_loop(self, task_support_set):\n",
    "\n",
    "        X_train, y_train = task_support_set\n",
    "\n",
    "        for t_epoch in range(self.inner_steps):\n",
    "            self.model.train()\n",
    "            preds = self.model(X_train)\n",
    "            loss = nn.CrossEntropyLoss()(preds, y_train)\n",
    "            self.inner_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.inner_optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def outer_loop(self):\n",
    "\n",
    "        meta_weights = {name: param.clone() for name, param in self.model.named_parameters() if 'running_mean' not in name and 'running_var' not in name}\n",
    "        task_losses = 0.0\n",
    "        for task in range(self.tasks):\n",
    "            if task%1==0:\n",
    "                print(\"\\nWorking with task: {}\".format(task + 1))\n",
    "            support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch = self.dataloaders[task]\n",
    "\n",
    "            if len(query_set_X_batch.shape) == 5:\n",
    "              support_set_X_batch = support_set_X_batch.squeeze(0)\n",
    "\n",
    "            if len(query_set_X_batch.shape) == 5:\n",
    "              query_set_X_batch = query_set_X_batch.squeeze(0)\n",
    "\n",
    "            support_set_X_batch =support_set_X_batch.to(self.device)\n",
    "            support_set_y_batch = support_set_y_batch.to(self.device)\n",
    "            query_set_X_batch = query_set_X_batch.to(self.device)\n",
    "            query_set_y_batch = query_set_y_batch.to(self.device)\n",
    "            #print(\"shapes support set \", support_set_X_batch.shape, support_set_y_batch.shape, sep = ' : ')\n",
    "            task_losses += self.inner_loop((support_set_X_batch, support_set_y_batch))\n",
    "\n",
    "            for name, param in self.model.named_parameters():\n",
    "              if 'running_mean' not in name and 'running_var' not in name:\n",
    "                    meta_weights[name] += (param - meta_weights[name]) * self.meta_lr\n",
    "\n",
    "        self.model.train()\n",
    "        self.model.load_state_dict(meta_weights, strict=False)\n",
    "\n",
    "\n",
    "        self.model.load_state_dict(meta_weights)\n",
    "        return task_losses/self.tasks\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        print('Start training...')\n",
    "        best_loss = float('inf')\n",
    "        best_model_weights = None\n",
    "        meta_losses = []\n",
    "        try:\n",
    "          for epoch in tqdm(range(self.epochs)):\n",
    "\n",
    "              meta_loss = self.outer_loop()\n",
    "              meta_losses.append(meta_loss)\n",
    "              if epoch % 1 == 0:\n",
    "                  print('Epoch: %d, Meta Loss: ', epoch, meta_loss)\n",
    "              if meta_loss < best_loss:\n",
    "                  best_loss = meta_loss\n",
    "                  best_model_weights = self.model.state_dict()\n",
    "\n",
    "          torch.save(best_model_weights, '{self.name}_best_model.pth')\n",
    "          print('Training finished.')\n",
    "        except KeyboardInterrupt:\n",
    "          torch.save(self.model.state_dict(), '{self.name}REPTILE_latest_model_interrupted_loss{meta_loss:.3f}.pth')\n",
    "          if best_model_weights:\n",
    "            torch.save(best_model_weights, '{self.name}REPTILE_best_model.pth')\n",
    "        finally:\n",
    "          plt.plot(meta_losses)\n",
    "          plt.xlabel('Epoch')\n",
    "          plt.ylabel('Loss')\n",
    "          plt.title('Meta Training Loss')\n",
    "          plt.show()\n",
    "\n",
    "        \n",
    "    def test(self, test_dataloader, pretrained_model = None):\n",
    "        # Initialize a copy of the model with the meta-learned weights\n",
    "        \n",
    "        if pretrained_model:\n",
    "            state_dict = torch.load(pretrained_model)\n",
    "            self.model.load_state_dict(state_dict)\n",
    "        test_model = copy.deepcopy(self.model)\n",
    "        # Move to the appropriate device\n",
    "        test_model = test_model.to(self.device)\n",
    "        print(\"Loaded model ...\")\n",
    "        # Initialize an optimizer for the inner-loop adaptation steps\n",
    "        test_optimizer = torch.optim.SGD(test_model.parameters(), lr=self.inner_lr)\n",
    "\n",
    "        support_X, support_y, query_X, query_y = test_dataloader[0]\n",
    "\n",
    "        # Move to the appropriate device\n",
    "        support_X, support_y = support_X.to(self.device), support_y.to(self.device)\n",
    "        query_X, query_y = query_X.to(self.device), query_y.to(self.device)\n",
    "\n",
    "        # Perform inner-loop adaptation steps\n",
    "        print(\"Performing inner loop ...\")\n",
    "        for _ in range(self.inner_steps):\n",
    "            support_preds = test_model(support_X)\n",
    "            loss = nn.CrossEntropyLoss()(support_preds, support_y)\n",
    "            test_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            test_optimizer.step()\n",
    "\n",
    "        # Evaluate on the query set\n",
    "        print(\" Predicting on query set ...\")\n",
    "        preds = test_model(query_X)\n",
    "        loss = nn.CrossEntropyLoss()(preds, query_y)\n",
    "        print(f'Test loss is {loss.item()}')\n",
    "        _, preds = torch.max(preds, dim=1)\n",
    "        accuracy = accuracy_score(query_y.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "        print(f\"Accuracy on test task: {accuracy}\")\n",
    "        conf_mat = confusion_matrix(query_y.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "        print(f'Confusion Matrix on test task: \\n{conf_mat}')\n",
    "        confusion_mat(query_y.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "    '''\n",
    "    def test(self, test_dataloader, pretrained_model = None):\n",
    "\n",
    "      if pretrained_model:\n",
    "        state_dict = torch.load(pretrained_model)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "      fast_weights = deepcopy(self.model.state_dict())\n",
    "      for task in range(len(test_dataloader)):\n",
    "\n",
    "          support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch = test_dataloader[task]\n",
    "          if len(query_set_X_batch.shape) == 5:\n",
    "              support_set_X_batch = support_set_X_batch.squeeze(0)\n",
    "\n",
    "          if len(query_set_X_batch.shape) == 5:\n",
    "            query_set_X_batch = query_set_X_batch.squeeze(0)\n",
    "\n",
    "          support_set_X_batch =support_set_X_batch.to(self.device)\n",
    "          support_set_y_batch = support_set_y_batch.to(self.device)\n",
    "          query_set_X_batch = query_set_X_batch.to(self.device)\n",
    "          query_set_y_batch = query_set_y_batch.to(self.device)\n",
    "\n",
    "          model_copy = self.inner_loop((support_set_X_batch, support_set_y_batch))\n",
    "\n",
    "          model_copy.eval()\n",
    "          with torch.no_grad():\n",
    "              preds = model_copy(query_set_X_batch)\n",
    "              _, preds = torch.max(preds, dim=1)\n",
    "              accuracy = accuracy_score(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "              print(f\"Accuracy on test task: {accuracy}\")\n",
    "              conf_mat = confusion_matrix(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "              print(f'Confusion Matrix on test task: \\n{conf_mat}')\n",
    "              confusion_mat(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "\n",
    "      self.model.train()  # Set the model back to training mode'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "344a8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReptileClone:\n",
    "    def __init__(self, name, model,dataloaders, test_dataloader=None, tasks=100, n_shot=3, epochs=50, inner_lr=0.01, meta_lr=0.001, inner_steps=10, device = 'cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        #print(self.model)\n",
    "        #print(self.device)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.name = name\n",
    "        self.tasks = tasks\n",
    "        self.n_shot = n_shot\n",
    "        self.epochs = epochs\n",
    "        self.dataloaders = dataloaders\n",
    "        #self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr_outer)\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.inner_optimizer = optim.SGD(self.model.parameters(), lr=self.inner_lr)\n",
    "\n",
    "    def inner_loop(self, task_support_set):\n",
    "\n",
    "        X_train, y_train = task_support_set\n",
    "\n",
    "        for t_epoch in range(self.inner_steps):\n",
    "            self.model.train()\n",
    "            preds = self.model(X_train)\n",
    "            loss = nn.CrossEntropyLoss()(preds, y_train)\n",
    "            self.inner_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.inner_optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def outer_loop(self):\n",
    "\n",
    "        meta_weights = {name: param.clone() for name, param in self.model.named_parameters() if 'running_mean' not in name and 'running_var' not in name}\n",
    "        task_losses = 0.0\n",
    "        update_directions = {name: 0 for name, _ in meta_weights.items()}\n",
    "        \n",
    "        for task in range(self.tasks):\n",
    "            if task%1==0:\n",
    "                print(\"\\nWorking with task: {}\".format(task + 1))\n",
    "            support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch = self.dataloaders[task]\n",
    "\n",
    "            if len(query_set_X_batch.shape) == 5:\n",
    "              support_set_X_batch = support_set_X_batch.squeeze(0)\n",
    "\n",
    "            if len(query_set_X_batch.shape) == 5:\n",
    "              query_set_X_batch = query_set_X_batch.squeeze(0)\n",
    "\n",
    "            support_set_X_batch =support_set_X_batch.to(self.device)\n",
    "            support_set_y_batch = support_set_y_batch.to(self.device)\n",
    "            query_set_X_batch = query_set_X_batch.to(self.device)\n",
    "            query_set_y_batch = query_set_y_batch.to(self.device)\n",
    "            #print(\"shapes support set \", support_set_X_batch.shape, support_set_y_batch.shape, sep = ' : ')\n",
    "            task_losses += self.inner_loop((support_set_X_batch, support_set_y_batch))\n",
    "\n",
    "            for name, param in self.model.named_parameters():\n",
    "              if 'running_mean' not in name and 'running_var' not in name:\n",
    "                    update_directions[name] += (param.detach() - meta_weights[name])\n",
    "        \n",
    "        update_directions = {name: direction / self.tasks for name, direction in update_directions.items()}\n",
    "        # Apply meta update\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if 'running_mean' not in name and 'running_var' not in name:\n",
    "                    param += self.meta_lr * update_directions[name]\n",
    "        \n",
    "        self.model.train()\n",
    "        self.model.load_state_dict(meta_weights)\n",
    "        return task_losses/self.tasks\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        print('Start training...')\n",
    "        best_loss = float('inf')\n",
    "        best_model_weights = None\n",
    "        meta_losses = []\n",
    "        try:\n",
    "          for epoch in tqdm(range(self.epochs)):\n",
    "\n",
    "              meta_loss = self.outer_loop()\n",
    "              meta_losses.append(meta_loss)\n",
    "              if epoch % 1 == 0:\n",
    "                  print('Epoch: %d, Meta Loss: ', epoch, meta_loss)\n",
    "              if meta_loss < best_loss:\n",
    "                  best_loss = meta_loss\n",
    "                  best_model_weights = self.model.state_dict()\n",
    "\n",
    "          torch.save(best_model_weights, '{self.name}_best_model.pth')\n",
    "          print('Training finished.')\n",
    "        except KeyboardInterrupt:\n",
    "          torch.save(self.model.state_dict(), '{self.name}REPTILE_latest_model_interrupted_loss{meta_loss:.3f}.pth')\n",
    "          if best_model_weights:\n",
    "            torch.save(best_model_weights, '{self.name}REPTILE_best_model.pth')\n",
    "        finally:\n",
    "          plt.plot(meta_losses)\n",
    "          plt.xlabel('Epoch')\n",
    "          plt.ylabel('Loss')\n",
    "          plt.title('Meta Training Loss')\n",
    "          plt.show()\n",
    "\n",
    "    def test(self, test_dataloader, pretrained_model = None):\n",
    "\n",
    "      if pretrained_model:\n",
    "        state_dict = torch.load(pretrained_model)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "      fast_weights = deepcopy(self.model.state_dict())\n",
    "      for task in range(len(test_dataloader)):\n",
    "\n",
    "          support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch = test_dataloader[task]\n",
    "          if len(query_set_X_batch.shape) == 5:\n",
    "              support_set_X_batch = support_set_X_batch.squeeze(0)\n",
    "\n",
    "          if len(query_set_X_batch.shape) == 5:\n",
    "            query_set_X_batch = query_set_X_batch.squeeze(0)\n",
    "\n",
    "          support_set_X_batch =support_set_X_batch.to(self.device)\n",
    "          support_set_y_batch = support_set_y_batch.to(self.device)\n",
    "          query_set_X_batch = query_set_X_batch.to(self.device)\n",
    "          query_set_y_batch = query_set_y_batch.to(self.device)\n",
    "\n",
    "          model_copy = self.inner_loop((support_set_X_batch, support_set_y_batch))\n",
    "\n",
    "          model_copy.eval()\n",
    "          with torch.no_grad():\n",
    "              preds = model_copy(query_set_X_batch)\n",
    "              _, preds = torch.max(preds, dim=1)\n",
    "              accuracy = accuracy_score(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "              print(f\"Accuracy on test task: {accuracy}\")\n",
    "              conf_mat = confusion_matrix(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "              print(f'Confusion Matrix on test task: \\n{conf_mat}')\n",
    "              confusion_mat(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "\n",
    "      self.model.train()  # Set the model back to training mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a74617",
   "metadata": {},
   "source": [
    "## REPTILE RUN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "427e23c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Working with task: 1\n",
      "\n",
      "Working with task: 2\n",
      "\n",
      "Working with task: 3\n",
      "\n",
      "Working with task: 4\n",
      "\n",
      "Working with task: 5\n",
      "\n",
      "Working with task: 6\n",
      "\n",
      "Working with task: 7\n",
      "\n",
      "Working with task: 8\n",
      "\n",
      "Working with task: 9\n",
      "\n",
      "Working with task: 10\n",
      "\n",
      "Working with task: 11\n",
      "\n",
      "Working with task: 12\n",
      "\n",
      "Working with task: 13\n",
      "\n",
      "Working with task: 14\n",
      "\n",
      "Working with task: 15\n",
      "\n",
      "Working with task: 16\n",
      "\n",
      "Working with task: 17\n",
      "\n",
      "Working with task: 18\n",
      "\n",
      "Working with task: 19\n",
      "\n",
      "Working with task: 20\n",
      "\n",
      "Working with task: 21\n",
      "\n",
      "Working with task: 22\n",
      "\n",
      "Working with task: 23\n",
      "\n",
      "Working with task: 24\n",
      "\n",
      "Working with task: 25\n",
      "\n",
      "Working with task: 26\n",
      "\n",
      "Working with task: 27\n",
      "\n",
      "Working with task: 28\n",
      "\n",
      "Working with task: 29\n",
      "\n",
      "Working with task: 30\n",
      "\n",
      "Working with task: 31\n",
      "\n",
      "Working with task: 32\n",
      "\n",
      "Working with task: 33\n",
      "\n",
      "Working with task: 34\n",
      "\n",
      "Working with task: 35\n",
      "\n",
      "Working with task: 36\n",
      "\n",
      "Working with task: 37\n",
      "\n",
      "Working with task: 38\n",
      "\n",
      "Working with task: 39\n",
      "\n",
      "Working with task: 40\n",
      "\n",
      "Working with task: 41\n",
      "\n",
      "Working with task: 42\n",
      "\n",
      "Working with task: 43\n",
      "\n",
      "Working with task: 44\n",
      "\n",
      "Working with task: 45\n",
      "\n",
      "Working with task: 46\n",
      "\n",
      "Working with task: 47\n",
      "\n",
      "Working with task: 48\n",
      "\n",
      "Working with task: 49\n",
      "\n",
      "Working with task: 50\n",
      "\n",
      "Working with task: 51\n",
      "\n",
      "Working with task: 52\n",
      "\n",
      "Working with task: 53\n",
      "\n",
      "Working with task: 54\n",
      "\n",
      "Working with task: 55\n",
      "\n",
      "Working with task: 56\n",
      "\n",
      "Working with task: 57\n",
      "\n",
      "Working with task: 58\n",
      "\n",
      "Working with task: 59\n",
      "\n",
      "Working with task: 60\n",
      "\n",
      "Working with task: 61\n",
      "\n",
      "Working with task: 62\n",
      "\n",
      "Working with task: 63\n",
      "\n",
      "Working with task: 64\n",
      "\n",
      "Working with task: 65\n",
      "\n",
      "Working with task: 66\n",
      "\n",
      "Working with task: 67\n",
      "\n",
      "Working with task: 68\n",
      "\n",
      "Working with task: 69\n",
      "\n",
      "Working with task: 70\n",
      "\n",
      "Working with task: 71\n",
      "\n",
      "Working with task: 72\n",
      "\n",
      "Working with task: 73\n",
      "\n",
      "Working with task: 74\n",
      "\n",
      "Working with task: 75\n",
      "\n",
      "Working with task: 76\n",
      "\n",
      "Working with task: 77\n",
      "\n",
      "Working with task: 78\n",
      "\n",
      "Working with task: 79\n",
      "\n",
      "Working with task: 80\n",
      "\n",
      "Working with task: 81\n",
      "\n",
      "Working with task: 82\n",
      "\n",
      "Working with task: 83\n",
      "\n",
      "Working with task: 84\n",
      "\n",
      "Working with task: 85\n",
      "\n",
      "Working with task: 86\n",
      "\n",
      "Working with task: 87\n",
      "\n",
      "Working with task: 88\n",
      "\n",
      "Working with task: 89\n",
      "\n",
      "Working with task: 90\n",
      "\n",
      "Working with task: 91\n",
      "\n",
      "Working with task: 92\n",
      "\n",
      "Working with task: 93\n",
      "\n",
      "Working with task: 94\n",
      "\n",
      "Working with task: 95\n",
      "\n",
      "Working with task: 96\n",
      "\n",
      "Working with task: 97\n",
      "\n",
      "Working with task: 98\n",
      "\n",
      "Working with task: 99\n",
      "\n",
      "Working with task: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/55 [10:02<9:01:59, 602.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: %d, Meta Loss:  0 1.120180937051773\n",
      "\n",
      "Working with task: 1\n",
      "\n",
      "Working with task: 2\n",
      "\n",
      "Working with task: 3\n",
      "\n",
      "Working with task: 4\n",
      "\n",
      "Working with task: 5\n",
      "\n",
      "Working with task: 6\n",
      "\n",
      "Working with task: 7\n",
      "\n",
      "Working with task: 8\n",
      "\n",
      "Working with task: 9\n",
      "\n",
      "Working with task: 10\n",
      "\n",
      "Working with task: 11\n",
      "\n",
      "Working with task: 12\n",
      "\n",
      "Working with task: 13\n",
      "\n",
      "Working with task: 14\n",
      "\n",
      "Working with task: 15\n",
      "\n",
      "Working with task: 16\n",
      "\n",
      "Working with task: 17\n",
      "\n",
      "Working with task: 18\n",
      "\n",
      "Working with task: 19\n",
      "\n",
      "Working with task: 20\n",
      "\n",
      "Working with task: 21\n",
      "\n",
      "Working with task: 22\n",
      "\n",
      "Working with task: 23\n",
      "\n",
      "Working with task: 24\n",
      "\n",
      "Working with task: 25\n",
      "\n",
      "Working with task: 26\n",
      "\n",
      "Working with task: 27\n",
      "\n",
      "Working with task: 28\n",
      "\n",
      "Working with task: 29\n",
      "\n",
      "Working with task: 30\n",
      "\n",
      "Working with task: 31\n",
      "\n",
      "Working with task: 32\n",
      "\n",
      "Working with task: 33\n",
      "\n",
      "Working with task: 34\n",
      "\n",
      "Working with task: 35\n",
      "\n",
      "Working with task: 36\n",
      "\n",
      "Working with task: 37\n",
      "\n",
      "Working with task: 38\n",
      "\n",
      "Working with task: 39\n",
      "\n",
      "Working with task: 40\n",
      "\n",
      "Working with task: 41\n",
      "\n",
      "Working with task: 42\n",
      "\n",
      "Working with task: 43\n",
      "\n",
      "Working with task: 44\n",
      "\n",
      "Working with task: 45\n",
      "\n",
      "Working with task: 46\n",
      "\n",
      "Working with task: 47\n",
      "\n",
      "Working with task: 48\n",
      "\n",
      "Working with task: 49\n",
      "\n",
      "Working with task: 50\n",
      "\n",
      "Working with task: 51\n",
      "\n",
      "Working with task: 52\n",
      "\n",
      "Working with task: 53\n",
      "\n",
      "Working with task: 54\n",
      "\n",
      "Working with task: 55\n",
      "\n",
      "Working with task: 56\n",
      "\n",
      "Working with task: 57\n",
      "\n",
      "Working with task: 58\n",
      "\n",
      "Working with task: 59\n",
      "\n",
      "Working with task: 60\n",
      "\n",
      "Working with task: 61\n",
      "\n",
      "Working with task: 62\n",
      "\n",
      "Working with task: 63\n",
      "\n",
      "Working with task: 64\n",
      "\n",
      "Working with task: 65\n",
      "\n",
      "Working with task: 66\n",
      "\n",
      "Working with task: 67\n",
      "\n",
      "Working with task: 68\n",
      "\n",
      "Working with task: 69\n",
      "\n",
      "Working with task: 70\n",
      "\n",
      "Working with task: 71\n",
      "\n",
      "Working with task: 72\n",
      "\n",
      "Working with task: 73\n",
      "\n",
      "Working with task: 74\n",
      "\n",
      "Working with task: 75\n",
      "\n",
      "Working with task: 76\n",
      "\n",
      "Working with task: 77\n",
      "\n",
      "Working with task: 78\n",
      "\n",
      "Working with task: 79\n",
      "\n",
      "Working with task: 80\n",
      "\n",
      "Working with task: 81\n",
      "\n",
      "Working with task: 82\n",
      "\n",
      "Working with task: 83\n",
      "\n",
      "Working with task: 84\n",
      "\n",
      "Working with task: 85\n",
      "\n",
      "Working with task: 86\n",
      "\n",
      "Working with task: 87\n",
      "\n",
      "Working with task: 88\n",
      "\n",
      "Working with task: 89\n",
      "\n",
      "Working with task: 90\n",
      "\n",
      "Working with task: 91\n",
      "\n",
      "Working with task: 92\n",
      "\n",
      "Working with task: 93\n",
      "\n",
      "Working with task: 94\n",
      "\n",
      "Working with task: 95\n",
      "\n",
      "Working with task: 96\n",
      "\n",
      "Working with task: 97\n",
      "\n",
      "Working with task: 98\n",
      "\n",
      "Working with task: 99\n",
      "\n",
      "Working with task: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 2/55 [19:59<8:49:10, 599.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: %d, Meta Loss:  1 1.1114804422855378\n",
      "\n",
      "Working with task: 1\n",
      "\n",
      "Working with task: 2\n",
      "\n",
      "Working with task: 3\n",
      "\n",
      "Working with task: 4\n",
      "\n",
      "Working with task: 5\n",
      "\n",
      "Working with task: 6\n",
      "\n",
      "Working with task: 7\n",
      "\n",
      "Working with task: 8\n",
      "\n",
      "Working with task: 9\n",
      "\n",
      "Working with task: 10\n",
      "\n",
      "Working with task: 11\n",
      "\n",
      "Working with task: 12\n",
      "\n",
      "Working with task: 13\n",
      "\n",
      "Working with task: 14\n",
      "\n",
      "Working with task: 15\n",
      "\n",
      "Working with task: 16\n",
      "\n",
      "Working with task: 17\n",
      "\n",
      "Working with task: 18\n",
      "\n",
      "Working with task: 19\n",
      "\n",
      "Working with task: 20\n",
      "\n",
      "Working with task: 21\n",
      "\n",
      "Working with task: 22\n",
      "\n",
      "Working with task: 23\n",
      "\n",
      "Working with task: 24\n",
      "\n",
      "Working with task: 25\n",
      "\n",
      "Working with task: 26\n",
      "\n",
      "Working with task: 27\n",
      "\n",
      "Working with task: 28\n",
      "\n",
      "Working with task: 29\n",
      "\n",
      "Working with task: 30\n",
      "\n",
      "Working with task: 31\n",
      "\n",
      "Working with task: 32\n",
      "\n",
      "Working with task: 33\n",
      "\n",
      "Working with task: 34\n",
      "\n",
      "Working with task: 35\n",
      "\n",
      "Working with task: 36\n",
      "\n",
      "Working with task: 37\n",
      "\n",
      "Working with task: 38\n",
      "\n",
      "Working with task: 39\n",
      "\n",
      "Working with task: 40\n",
      "\n",
      "Working with task: 41\n",
      "\n",
      "Working with task: 42\n",
      "\n",
      "Working with task: 43\n",
      "\n",
      "Working with task: 44\n",
      "\n",
      "Working with task: 45\n",
      "\n",
      "Working with task: 46\n",
      "\n",
      "Working with task: 47\n",
      "\n",
      "Working with task: 48\n",
      "\n",
      "Working with task: 49\n",
      "\n",
      "Working with task: 50\n",
      "\n",
      "Working with task: 51\n",
      "\n",
      "Working with task: 52\n",
      "\n",
      "Working with task: 53\n",
      "\n",
      "Working with task: 54\n",
      "\n",
      "Working with task: 55\n",
      "\n",
      "Working with task: 56\n",
      "\n",
      "Working with task: 57\n",
      "\n",
      "Working with task: 58\n",
      "\n",
      "Working with task: 59\n",
      "\n",
      "Working with task: 60\n",
      "\n",
      "Working with task: 61\n",
      "\n",
      "Working with task: 62\n",
      "\n",
      "Working with task: 63\n",
      "\n",
      "Working with task: 64\n",
      "\n",
      "Working with task: 65\n",
      "\n",
      "Working with task: 66\n",
      "\n",
      "Working with task: 67\n",
      "\n",
      "Working with task: 68\n",
      "\n",
      "Working with task: 69\n",
      "\n",
      "Working with task: 70\n",
      "\n",
      "Working with task: 71\n",
      "\n",
      "Working with task: 72\n",
      "\n",
      "Working with task: 73\n",
      "\n",
      "Working with task: 74\n",
      "\n",
      "Working with task: 75\n",
      "\n",
      "Working with task: 76\n",
      "\n",
      "Working with task: 77\n",
      "\n",
      "Working with task: 78\n",
      "\n",
      "Working with task: 79\n",
      "\n",
      "Working with task: 80\n",
      "\n",
      "Working with task: 81\n",
      "\n",
      "Working with task: 82\n",
      "\n",
      "Working with task: 83\n",
      "\n",
      "Working with task: 84\n",
      "\n",
      "Working with task: 85\n",
      "\n",
      "Working with task: 86\n",
      "\n",
      "Working with task: 87\n",
      "\n",
      "Working with task: 88\n",
      "\n",
      "Working with task: 89\n",
      "\n",
      "Working with task: 90\n",
      "\n",
      "Working with task: 91\n",
      "\n",
      "Working with task: 92\n",
      "\n",
      "Working with task: 93\n",
      "\n",
      "Working with task: 94\n",
      "\n",
      "Working with task: 95\n",
      "\n",
      "Working with task: 96\n",
      "\n",
      "Working with task: 97\n",
      "\n",
      "Working with task: 98\n",
      "\n",
      "Working with task: 99\n",
      "\n",
      "Working with task: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/55 [29:55<8:38:13, 597.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: %d, Meta Loss:  2 1.1090285450220108\n",
      "\n",
      "Working with task: 1\n",
      "\n",
      "Working with task: 2\n",
      "\n",
      "Working with task: 3\n",
      "\n",
      "Working with task: 4\n",
      "\n",
      "Working with task: 5\n",
      "\n",
      "Working with task: 6\n",
      "\n",
      "Working with task: 7\n",
      "\n",
      "Working with task: 8\n",
      "\n",
      "Working with task: 9\n",
      "\n",
      "Working with task: 10\n",
      "\n",
      "Working with task: 11\n",
      "\n",
      "Working with task: 12\n",
      "\n",
      "Working with task: 13\n",
      "\n",
      "Working with task: 14\n",
      "\n",
      "Working with task: 15\n",
      "\n",
      "Working with task: 16\n",
      "\n",
      "Working with task: 17\n",
      "\n",
      "Working with task: 18\n",
      "\n",
      "Working with task: 19\n",
      "\n",
      "Working with task: 20\n",
      "\n",
      "Working with task: 21\n",
      "\n",
      "Working with task: 22\n",
      "\n",
      "Working with task: 23\n",
      "\n",
      "Working with task: 24\n",
      "\n",
      "Working with task: 25\n",
      "\n",
      "Working with task: 26\n",
      "\n",
      "Working with task: 27\n",
      "\n",
      "Working with task: 28\n",
      "\n",
      "Working with task: 29\n",
      "\n",
      "Working with task: 30\n",
      "\n",
      "Working with task: 31\n",
      "\n",
      "Working with task: 32\n",
      "\n",
      "Working with task: 33\n",
      "\n",
      "Working with task: 34\n",
      "\n",
      "Working with task: 35\n",
      "\n",
      "Working with task: 36\n",
      "\n",
      "Working with task: 37\n",
      "\n",
      "Working with task: 38\n",
      "\n",
      "Working with task: 39\n",
      "\n",
      "Working with task: 40\n",
      "\n",
      "Working with task: 41\n",
      "\n",
      "Working with task: 42\n",
      "\n",
      "Working with task: 43\n",
      "\n",
      "Working with task: 44\n",
      "\n",
      "Working with task: 45\n",
      "\n",
      "Working with task: 46\n",
      "\n",
      "Working with task: 47\n",
      "\n",
      "Working with task: 48\n",
      "\n",
      "Working with task: 49\n",
      "\n",
      "Working with task: 50\n",
      "\n",
      "Working with task: 51\n",
      "\n",
      "Working with task: 52\n",
      "\n",
      "Working with task: 53\n",
      "\n",
      "Working with task: 54\n",
      "\n",
      "Working with task: 55\n",
      "\n",
      "Working with task: 56\n",
      "\n",
      "Working with task: 57\n",
      "\n",
      "Working with task: 58\n",
      "\n",
      "Working with task: 59\n",
      "\n",
      "Working with task: 60\n",
      "\n",
      "Working with task: 61\n",
      "\n",
      "Working with task: 62\n",
      "\n",
      "Working with task: 63\n",
      "\n",
      "Working with task: 64\n",
      "\n",
      "Working with task: 65\n",
      "\n",
      "Working with task: 66\n",
      "\n",
      "Working with task: 67\n",
      "\n",
      "Working with task: 68\n",
      "\n",
      "Working with task: 69\n",
      "\n",
      "Working with task: 70\n",
      "\n",
      "Working with task: 71\n",
      "\n",
      "Working with task: 72\n",
      "\n",
      "Working with task: 73\n",
      "\n",
      "Working with task: 74\n",
      "\n",
      "Working with task: 75\n",
      "\n",
      "Working with task: 76\n",
      "\n",
      "Working with task: 77\n",
      "\n",
      "Working with task: 78\n",
      "\n",
      "Working with task: 79\n",
      "\n",
      "Working with task: 80\n",
      "\n",
      "Working with task: 81\n",
      "\n",
      "Working with task: 82\n",
      "\n",
      "Working with task: 83\n",
      "\n",
      "Working with task: 84\n",
      "\n",
      "Working with task: 85\n",
      "\n",
      "Working with task: 86\n",
      "\n",
      "Working with task: 87\n",
      "\n",
      "Working with task: 88\n",
      "\n",
      "Working with task: 89\n",
      "\n",
      "Working with task: 90\n",
      "\n",
      "Working with task: 91\n",
      "\n",
      "Working with task: 92\n",
      "\n",
      "Working with task: 93\n",
      "\n",
      "Working with task: 94\n",
      "\n",
      "Working with task: 95\n",
      "\n",
      "Working with task: 96\n",
      "\n",
      "Working with task: 97\n",
      "\n",
      "Working with task: 98\n",
      "\n",
      "Working with task: 99\n",
      "\n",
      "Working with task: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/55 [39:57<8:29:31, 599.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: %d, Meta Loss:  3 1.108683078289032\n",
      "\n",
      "Working with task: 1\n",
      "\n",
      "Working with task: 2\n",
      "\n",
      "Working with task: 3\n",
      "\n",
      "Working with task: 4\n",
      "\n",
      "Working with task: 5\n",
      "\n",
      "Working with task: 6\n",
      "\n",
      "Working with task: 7\n",
      "\n",
      "Working with task: 8\n",
      "\n",
      "Working with task: 9\n",
      "\n",
      "Working with task: 10\n",
      "\n",
      "Working with task: 11\n",
      "\n",
      "Working with task: 12\n",
      "\n",
      "Working with task: 13\n",
      "\n",
      "Working with task: 14\n",
      "\n",
      "Working with task: 15\n",
      "\n",
      "Working with task: 16\n",
      "\n",
      "Working with task: 17\n",
      "\n",
      "Working with task: 18\n",
      "\n",
      "Working with task: 19\n",
      "\n",
      "Working with task: 20\n",
      "\n",
      "Working with task: 21\n",
      "\n",
      "Working with task: 22\n",
      "\n",
      "Working with task: 23\n",
      "\n",
      "Working with task: 24\n",
      "\n",
      "Working with task: 25\n",
      "\n",
      "Working with task: 26\n",
      "\n",
      "Working with task: 27\n",
      "\n",
      "Working with task: 28\n",
      "\n",
      "Working with task: 29\n",
      "\n",
      "Working with task: 30\n",
      "\n",
      "Working with task: 31\n",
      "\n",
      "Working with task: 32\n",
      "\n",
      "Working with task: 33\n",
      "\n",
      "Working with task: 34\n",
      "\n",
      "Working with task: 35\n",
      "\n",
      "Working with task: 36\n",
      "\n",
      "Working with task: 37\n",
      "\n",
      "Working with task: 38\n",
      "\n",
      "Working with task: 39\n",
      "\n",
      "Working with task: 40\n",
      "\n",
      "Working with task: 41\n",
      "\n",
      "Working with task: 42\n",
      "\n",
      "Working with task: 43\n",
      "\n",
      "Working with task: 44\n",
      "\n",
      "Working with task: 45\n",
      "\n",
      "Working with task: 46\n",
      "\n",
      "Working with task: 47\n",
      "\n",
      "Working with task: 48\n",
      "\n",
      "Working with task: 49\n",
      "\n",
      "Working with task: 50\n",
      "\n",
      "Working with task: 51\n",
      "\n",
      "Working with task: 52\n",
      "\n",
      "Working with task: 53\n",
      "\n",
      "Working with task: 54\n",
      "\n",
      "Working with task: 55\n",
      "\n",
      "Working with task: 56\n",
      "\n",
      "Working with task: 57\n",
      "\n",
      "Working with task: 58\n",
      "\n",
      "Working with task: 59\n",
      "\n",
      "Working with task: 60\n",
      "\n",
      "Working with task: 61\n",
      "\n",
      "Working with task: 62\n",
      "\n",
      "Working with task: 63\n",
      "\n",
      "Working with task: 64\n",
      "\n",
      "Working with task: 65\n",
      "\n",
      "Working with task: 66\n",
      "\n",
      "Working with task: 67\n",
      "\n",
      "Working with task: 68\n",
      "\n",
      "Working with task: 69\n",
      "\n",
      "Working with task: 70\n",
      "\n",
      "Working with task: 71\n",
      "\n",
      "Working with task: 72\n",
      "\n",
      "Working with task: 73\n",
      "\n",
      "Working with task: 74\n",
      "\n",
      "Working with task: 75\n",
      "\n",
      "Working with task: 76\n",
      "\n",
      "Working with task: 77\n",
      "\n",
      "Working with task: 78\n",
      "\n",
      "Working with task: 79\n",
      "\n",
      "Working with task: 80\n",
      "\n",
      "Working with task: 81\n",
      "\n",
      "Working with task: 82\n",
      "\n",
      "Working with task: 83\n",
      "\n",
      "Working with task: 84\n",
      "\n",
      "Working with task: 85\n",
      "\n",
      "Working with task: 86\n",
      "\n",
      "Working with task: 87\n",
      "\n",
      "Working with task: 88\n",
      "\n",
      "Working with task: 89\n",
      "\n",
      "Working with task: 90\n",
      "\n",
      "Working with task: 91\n",
      "\n",
      "Working with task: 92\n",
      "\n",
      "Working with task: 93\n",
      "\n",
      "Working with task: 94\n",
      "\n",
      "Working with task: 95\n",
      "\n",
      "Working with task: 96\n",
      "\n",
      "Working with task: 97\n",
      "\n",
      "Working with task: 98\n",
      "\n",
      "Working with task: 99\n",
      "\n",
      "Working with task: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5/55 [49:58<8:19:57, 599.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: %d, Meta Loss:  4 1.110585018992424\n",
      "\n",
      "Working with task: 1\n",
      "\n",
      "Working with task: 2\n",
      "\n",
      "Working with task: 3\n",
      "\n",
      "Working with task: 4\n",
      "\n",
      "Working with task: 5\n",
      "\n",
      "Working with task: 6\n",
      "\n",
      "Working with task: 7\n",
      "\n",
      "Working with task: 8\n",
      "\n",
      "Working with task: 9\n",
      "\n",
      "Working with task: 10\n",
      "\n",
      "Working with task: 11\n",
      "\n",
      "Working with task: 12\n",
      "\n",
      "Working with task: 13\n",
      "\n",
      "Working with task: 14\n",
      "\n",
      "Working with task: 15\n",
      "\n",
      "Working with task: 16\n",
      "\n",
      "Working with task: 17\n",
      "\n",
      "Working with task: 18\n",
      "\n",
      "Working with task: 19\n",
      "\n",
      "Working with task: 20\n",
      "\n",
      "Working with task: 21\n",
      "\n",
      "Working with task: 22\n",
      "\n",
      "Working with task: 23\n",
      "\n",
      "Working with task: 24\n",
      "\n",
      "Working with task: 25\n",
      "\n",
      "Working with task: 26\n",
      "\n",
      "Working with task: 27\n",
      "\n",
      "Working with task: 28\n",
      "\n",
      "Working with task: 29\n",
      "\n",
      "Working with task: 30\n",
      "\n",
      "Working with task: 31\n",
      "\n",
      "Working with task: 32\n",
      "\n",
      "Working with task: 33\n",
      "\n",
      "Working with task: 34\n",
      "\n",
      "Working with task: 35\n",
      "\n",
      "Working with task: 36\n",
      "\n",
      "Working with task: 37\n",
      "\n",
      "Working with task: 38\n",
      "\n",
      "Working with task: 39\n",
      "\n",
      "Working with task: 40\n",
      "\n",
      "Working with task: 41\n",
      "\n",
      "Working with task: 42\n",
      "\n",
      "Working with task: 43\n",
      "\n",
      "Working with task: 44\n",
      "\n",
      "Working with task: 45\n",
      "\n",
      "Working with task: 46\n",
      "\n",
      "Working with task: 47\n",
      "\n",
      "Working with task: 48\n",
      "\n",
      "Working with task: 49\n",
      "\n",
      "Working with task: 50\n",
      "\n",
      "Working with task: 51\n",
      "\n",
      "Working with task: 52\n",
      "\n",
      "Working with task: 53\n",
      "\n",
      "Working with task: 54\n",
      "\n",
      "Working with task: 55\n",
      "\n",
      "Working with task: 56\n",
      "\n",
      "Working with task: 57\n",
      "\n",
      "Working with task: 58\n",
      "\n",
      "Working with task: 59\n",
      "\n",
      "Working with task: 60\n",
      "\n",
      "Working with task: 61\n",
      "\n",
      "Working with task: 62\n",
      "\n",
      "Working with task: 63\n",
      "\n",
      "Working with task: 64\n",
      "\n",
      "Working with task: 65\n",
      "\n",
      "Working with task: 66\n",
      "\n",
      "Working with task: 67\n",
      "\n",
      "Working with task: 68\n",
      "\n",
      "Working with task: 69\n",
      "\n",
      "Working with task: 70\n",
      "\n",
      "Working with task: 71\n",
      "\n",
      "Working with task: 72\n",
      "\n",
      "Working with task: 73\n",
      "\n",
      "Working with task: 74\n",
      "\n",
      "Working with task: 75\n",
      "\n",
      "Working with task: 76\n",
      "\n",
      "Working with task: 77\n",
      "\n",
      "Working with task: 78\n",
      "\n",
      "Working with task: 79\n",
      "\n",
      "Working with task: 80\n",
      "\n",
      "Working with task: 81\n",
      "\n",
      "Working with task: 82\n",
      "\n",
      "Working with task: 83\n",
      "\n",
      "Working with task: 84\n",
      "\n",
      "Working with task: 85\n",
      "\n",
      "Working with task: 86\n",
      "\n",
      "Working with task: 87\n",
      "\n",
      "Working with task: 88\n",
      "\n",
      "Working with task: 89\n",
      "\n",
      "Working with task: 90\n",
      "\n",
      "Working with task: 91\n",
      "\n",
      "Working with task: 92\n",
      "\n",
      "Working with task: 93\n",
      "\n",
      "Working with task: 94\n",
      "\n",
      "Working with task: 95\n",
      "\n",
      "Working with task: 96\n",
      "\n",
      "Working with task: 97\n",
      "\n",
      "Working with task: 98\n",
      "\n",
      "Working with task: 99\n",
      "\n",
      "Working with task: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6/55 [1:00:05<8:11:52, 602.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: %d, Meta Loss:  5 1.105757377743721\n",
      "\n",
      "Working with task: 1\n",
      "\n",
      "Working with task: 2\n",
      "\n",
      "Working with task: 3\n",
      "\n",
      "Working with task: 4\n",
      "\n",
      "Working with task: 5\n",
      "\n",
      "Working with task: 6\n",
      "\n",
      "Working with task: 7\n",
      "\n",
      "Working with task: 8\n",
      "\n",
      "Working with task: 9\n",
      "\n",
      "Working with task: 10\n",
      "\n",
      "Working with task: 11\n",
      "\n",
      "Working with task: 12\n",
      "\n",
      "Working with task: 13\n",
      "\n",
      "Working with task: 14\n",
      "\n",
      "Working with task: 15\n",
      "\n",
      "Working with task: 16\n",
      "\n",
      "Working with task: 17\n",
      "\n",
      "Working with task: 18\n",
      "\n",
      "Working with task: 19\n",
      "\n",
      "Working with task: 20\n",
      "\n",
      "Working with task: 21\n",
      "\n",
      "Working with task: 22\n",
      "\n",
      "Working with task: 23\n",
      "\n",
      "Working with task: 24\n",
      "\n",
      "Working with task: 25\n",
      "\n",
      "Working with task: 26\n",
      "\n",
      "Working with task: 27\n",
      "\n",
      "Working with task: 28\n",
      "\n",
      "Working with task: 29\n",
      "\n",
      "Working with task: 30\n",
      "\n",
      "Working with task: 31\n",
      "\n",
      "Working with task: 32\n",
      "\n",
      "Working with task: 33\n",
      "\n",
      "Working with task: 34\n",
      "\n",
      "Working with task: 35\n",
      "\n",
      "Working with task: 36\n",
      "\n",
      "Working with task: 37\n",
      "\n",
      "Working with task: 38\n",
      "\n",
      "Working with task: 39\n",
      "\n",
      "Working with task: 40\n",
      "\n",
      "Working with task: 41\n",
      "\n",
      "Working with task: 42\n",
      "\n",
      "Working with task: 43\n",
      "\n",
      "Working with task: 44\n",
      "\n",
      "Working with task: 45\n",
      "\n",
      "Working with task: 46\n",
      "\n",
      "Working with task: 47\n",
      "\n",
      "Working with task: 48\n",
      "\n",
      "Working with task: 49\n",
      "\n",
      "Working with task: 50\n",
      "\n",
      "Working with task: 51\n",
      "\n",
      "Working with task: 52\n",
      "\n",
      "Working with task: 53\n",
      "\n",
      "Working with task: 54\n",
      "\n",
      "Working with task: 55\n",
      "\n",
      "Working with task: 56\n",
      "\n",
      "Working with task: 57\n",
      "\n",
      "Working with task: 58\n",
      "\n",
      "Working with task: 59\n",
      "\n",
      "Working with task: 60\n",
      "\n",
      "Working with task: 61\n",
      "\n",
      "Working with task: 62\n",
      "\n",
      "Working with task: 63\n",
      "\n",
      "Working with task: 64\n",
      "\n",
      "Working with task: 65\n",
      "\n",
      "Working with task: 66\n",
      "\n",
      "Working with task: 67\n",
      "\n",
      "Working with task: 68\n",
      "\n",
      "Working with task: 69\n",
      "\n",
      "Working with task: 70\n",
      "\n",
      "Working with task: 71\n",
      "\n",
      "Working with task: 72\n",
      "\n",
      "Working with task: 73\n",
      "\n",
      "Working with task: 74\n",
      "\n",
      "Working with task: 75\n",
      "\n",
      "Working with task: 76\n",
      "\n",
      "Working with task: 77\n",
      "\n",
      "Working with task: 78\n",
      "\n",
      "Working with task: 79\n",
      "\n",
      "Working with task: 80\n",
      "\n",
      "Working with task: 81\n",
      "\n",
      "Working with task: 82\n",
      "\n",
      "Working with task: 83\n",
      "\n",
      "Working with task: 84\n",
      "\n",
      "Working with task: 85\n",
      "\n",
      "Working with task: 86\n",
      "\n",
      "Working with task: 87\n",
      "\n",
      "Working with task: 88\n",
      "\n",
      "Working with task: 89\n",
      "\n",
      "Working with task: 90\n",
      "\n",
      "Working with task: 91\n",
      "\n",
      "Working with task: 92\n",
      "\n",
      "Working with task: 93\n",
      "\n",
      "Working with task: 94\n",
      "\n",
      "Working with task: 95\n",
      "\n",
      "Working with task: 96\n",
      "\n",
      "Working with task: 97\n",
      "\n",
      "Working with task: 98\n",
      "\n",
      "Working with task: 99\n",
      "\n",
      "Working with task: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7/55 [1:10:02<8:00:39, 600.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: %d, Meta Loss:  6 1.1130941164493562\n",
      "\n",
      "Working with task: 1\n",
      "\n",
      "Working with task: 2\n",
      "\n",
      "Working with task: 3\n",
      "\n",
      "Working with task: 4\n",
      "\n",
      "Working with task: 5\n",
      "\n",
      "Working with task: 6\n",
      "\n",
      "Working with task: 7\n",
      "\n",
      "Working with task: 8\n",
      "\n",
      "Working with task: 9\n",
      "\n",
      "Working with task: 10\n",
      "\n",
      "Working with task: 11\n",
      "\n",
      "Working with task: 12\n",
      "\n",
      "Working with task: 13\n",
      "\n",
      "Working with task: 14\n",
      "\n",
      "Working with task: 15\n",
      "\n",
      "Working with task: 16\n",
      "\n",
      "Working with task: 17\n",
      "\n",
      "Working with task: 18\n",
      "\n",
      "Working with task: 19\n",
      "\n",
      "Working with task: 20\n",
      "\n",
      "Working with task: 21\n",
      "\n",
      "Working with task: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 7/55 [1:11:59<8:13:36, 617.02s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "try:\n",
    "  del reptile\n",
    "except:\n",
    "  pass\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "reptile = Reptile(name = 'run1', model = MyPretrainedAlexNet(),dataloaders=meta_learning_loader, test_dataloader=None, tasks=100, n_shot=3, epochs=55, inner_lr=0.0001, meta_lr=0.0001, inner_steps=2, device = device)\n",
    "reptile.train()\n",
    "#reptile.test(TestFewShotLoader, pretrained_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83d7d6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d1974dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_learning.ipynb\r\n",
      "meta_tasks_symlink\r\n",
      "{self.name}REPTILE_best_model.pth\r\n",
      "{self.name}REPTILE_latest_model_interrupted_loss{meta_loss:.3f}.pth\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dd67f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv {self.name}REPTILE_best_model.pth REPTILE_best_model.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "271aa003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model ...\n",
      "Performing inner loop ...\n",
      " Predicting on query set ...\n",
      "Test loss is 1.0872137546539307\n",
      "Accuracy on test task: 0.39444444444444443\n",
      "Confusion Matrix on test task: \n",
      "[[72 50 57]\n",
      " [40 55 38]\n",
      " [21 12 15]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi0AAAHxCAYAAACh/Yy7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAArUklEQVR4nO3deZhdZZXv8e8iJBBmAoQEZBBk0MuoyCRDAhJBseGiKC3SRKEjCsjUNgG9IjRTI9JqO9wOMkQEWlQmERluZMYGAjIJEhQEgUggQBiSkKRq3T/OTixiUlVJ6pyTt/b38zznqXP22efdq0KRWvm97947MhNJkqSl3TLtLkCSJKk3bFokSVIRbFokSVIRbFokSVIRbFokSVIRbFokSVIRbFqkpVREDI6IX0bEtIj42RKMc3BE3NSXtbVDRPw6Ig5tdx2S2semRVpCEfGZiJgYEW9GxOTql+sufTD0J4G1gTUy88DFHSQzL83MUX1QzztExIiIyIi4cr7tW1fbb+3lON+IiJ/0tF9m7pOZ4xezXEn9gE2LtAQi4njg28CZNBqM9YEfAPv1wfAbAJMyc04fjNUsLwE7R8QaXbYdCkzqqwNEg39XSbJpkRZXRKwKnAYcmZlXZuZbmTk7M3+ZmV+p9lkuIr4dES9Uj29HxHLVeyMi4rmIOCEiplQpzeeq904Fvg58ukpwDps/kYiIDatEY9nq9eiIeCoi3oiIpyPi4C7b7+zyuZ0j4r5q2um+iNi5y3u3RsS/RcRd1Tg3RcSa3fwxzAKuBg6qPj8A+BRw6Xx/Vt+JiL9ExOsRcX9E7Fpt3xs4ucv3+VCXOs6IiLuA6cBG1bbDq/d/GBE/7zL+v0fEhIiI3v73k1QemxZp8e0ELA9c1c0+XwV2BLYBtga2B77W5f1hwKrAusBhwPcjYvXMPIVGevPTzFwpMy/orpCIWBH4LrBPZq4M7Aw8uID9hgC/qvZdAzgP+NV8SclngM8BQ4FBwL90d2zgx8A/Vc8/AvweeGG+fe6j8WcwBLgM+FlELJ+ZN8z3fW7d5TOHAGOAlYFn5hvvBGCrqiHblcaf3aHpfUmkfs2mRVp8awAv9zB9czBwWmZOycyXgFNp/DKea3b1/uzMvB54E9hsMevpBLaIiMGZOTkzf7+AfT4GPJmZl2TmnMy8HPgD8PEu+1yUmZMycwZwBY1mY6Ey825gSERsRqN5+fEC9vlJZk6tjvktYDl6/j4vzszfV5+ZPd9404HP0mi6fgIcnZnP9TCepMLZtEiLbyqw5tzpmYVYh3emBM9U2+aNMV/TMx1YaVELycy3gE8DRwCTI+JXEbF5L+qZW9O6XV7/dTHquQQ4ChjJApKnagrs8WpK6jUa6VJ3004Af+nuzcy8F3gKCBrNlaR+zqZFWny/BWYC+3ezzws0FtTOtT5/P3XSW28BK3R5Pazrm5l5Y2buBQynkZ6c34t65tb0/GLWNNclwJeA66sUZJ5q+uZEGmtdVs/M1YBpNJoNgIVN6XQ71RMRR9JIbF4A/nWxK5dUDJsWaTFl5jQai2W/HxH7R8QKETEwIvaJiHOq3S4HvhYRa1ULWr9OYzpjcTwI7BYR61eLgE+a+0ZErB0R/1CtbXmbxjRTxwLGuB7YtDpNe9mI+DTwPuC6xawJgMx8Gtidxhqe+a0MzKFxptGyEfF1YJUu778IbLgoZwhFxKbA6TSmiA4B/jUitlm86iWVwqZFWgKZeR5wPI3FtS/RmNI4isYZNdD4xToReBh4BHig2rY4x7oZ+Gk11v28s9FYhsbi1BeAV2g0EF9awBhTgX2rfafSSCj2zcyXF6em+ca+MzMXlCLdCPyaxmnQz9BIp7pO/cy9cN7UiHigp+NU03E/Af49Mx/KzCdpnIF0ydwzsyT1T+Fie0mSVAKTFkmSVASbFkmSVASbFkmSVASbFkmSVITuLorVcuuuu66rgtVnbrzxxnaXoH5m4sSJ7S5B/czo0aNber+sZv2eff7551vyfZi0SJKkIti0SJKkIixV00OSJKl5llmm7KzCpkWSpJoovWkpu3pJklQbJi2SJNWESYskSVILmLRIklQTpSctNi2SJNVE6U1L2dVLkqTaMGmRJKkmTFokSZIWIiI2i4gHuzxej4hjI2JIRNwcEU9WX1fvaSybFkmSamKZZZZpyqM7mflEZm6TmdsAHwCmA1cBY4EJmbkJMKF63S2nhyRJqomlYHpoT+BPmflMROwHjKi2jwduBU7s7sNtr16SJJUtIsZExMQujzEL2fUg4PLq+dqZORmg+jq0p+OYtEiSVBPNSloycxwwrrt9ImIQ8A/ASYt7HJMWSZLUCvsAD2Tmi9XrFyNiOED1dUpPA5i0SJJUE21e0/KP/G1qCOBa4FDg7OrrNT0NYNIiSZKaKiJWAPYCruyy+Wxgr4h4snrv7J7GMWmRJKkm2pW0ZOZ0YI35tk2lcTZRr9m0SJJUE0vBKc9LpOzqJUlSbZi0SJJUEyYtkiRJLWDSIklSTZSetNi0SJJUE6U3LWVXL0mSasOkRZKkmjBpkSRJagGTFkmSaqL0pMWmRZKkmii9aSm7ekmSVBsmLZIk1YRJiyRJUguYtEiSVBMmLZIkSS1g0iJJUk2UnrTYtEiSVBOlNy1lVy9JkmrDpEWSpJowaZEkSWoBkxZJkmqi9KTFpkWSpJoovWkpu3pJklQbJi2SJNWESYskSVILmLRIklQTpSctNi2SJNXEgAED2l3CEim75ZIkSbVh0iJJUk2UPj1UdvWSJKk2TFokSaoJkxZJkqQWMGmRJKkmSj97yKZFkqSacHpIkiSpBUxaJEmqCZMWSZKkFjBpkSSpJkpPWmxaJEmqidLPHiq75ZIkSbVh0iJJUk2UPj1UdvWSJKk2TFokSaqJ0pMWmxZJkmqi9Kal7OolSVJtmLRIklQTnvIsSZLUAiYtS7GNN96YH/7wh/Ner7/++px77rkMGzaMvfbai1mzZvHMM89w/PHH8/rrr7exUpXkiCOOYPDgwSyzzDIMGDCAc845hzfeeIPzzjuPKVOmMHToUE444QRWWmmldpeqQnzyk59k9uzZZCadnZ1cd9117L777qy66qoADBo0iFmzZnHttde2uVKVvqalqU1LROwNfAcYAPwoM89u5vH6mz/96U+MGjUKaPyg3X///fz6179m44035qyzzqKjo4OTTz6Zo446ijPPPLPN1aokp556Kqusssq811dddRVbbrklBxxwAFdeeSVXXXUVhxxySBsrVGluuOEG3n777Xmvb7vttnnPt9tuO2bPnt2OsjSf0puWplUfEQOA7wP7AO8D/jEi3tes4/V3u+yyC8888wzPP/88t99+Ox0dHQA88MADDB8+vM3VqXT33XcfI0eOBGDkyJHce++9ba5I/cm73/1unnrqqXaXoX6gmUnL9sAfM/MpgIj4b2A/4LEmHrPf2m+//bj66qv/bvtBBx1k5KpFEhGcdtppRAR77bUXo0aN4rXXXmP11VcHYPXVV2fatGltrlIlyUxGjRpFZjJp0iQmTZo07721116bGTNm8MYbb7SxQs1V+kLcZjYt6wJ/6fL6OWCH+XeKiDHAGIBVV12VFVdcsYkllWngwIGMGjWKs8466x3bv/zlLzNnzhyuvPLKNlWmEp1xxhkMGTKEadOmceqpp7Luuuu2uyQV7vrrr2fGjBksv/zyjBo1imnTpvHiiy8CjZTl6aefbnOF6i+aObkVC9iWf7chc1xmbpeZ29mwLNjIkSN55JFHePnll+dtO/DAA/nwhz/MUUcd1cbKVKIhQ4YAjX8k7LDDDvzxj39ktdVW49VXXwXg1VdfnbeAUuqNGTNmADBz5kyeffZZ1lxzTaCR6m2wwQY2LUuRZZZZpimPltXfxLGfA9br8vpdwAtNPF6/tf/++79jamjEiBF86UtfYvTo0cycObN9hak4M2fOfMcvmIceeoj111+f7bbbjltuuQWAW265hQ9+8IPtLFMFWXbZZVl22WXnPV9nnXV47bXXAFhnnXWYNm0a06dPb2OF6k+aOT10H7BJRLwbeB44CPhME4/XLy2//PLstttunHjiifO2nX766Sy33HL893//N9BYjDt27Nh2laiCvPbaa5xzzjkAdHR0sOuuu7Ltttvynve8h29961tMmDCBtdZaixNOOKHNlaoUyy+/PHvssQfQSFaefvppnn/+ecCpoaVR6WcPRebfzdj03eARHwW+TeOU5wsz84zu9l933XWbV4xq58Ybb2x3CepnJk6c2O4S1M+MHj16QUspmubkk09uyu/ZM888syXfR1Ov05KZ1wPXN/MYkiSpHrwiriRJNVH6Kc9lT25JkqTaMGmRJKkmSl+Ia9MiSVJNlN60lF29JEmqDZMWSZJqol1JS0SsBvwI2ILG1fE/DzwB/BTYEPgz8KnMfLW7cUxaJElSs30HuCEzNwe2Bh4HxgITMnMTYEL1ulsmLZIk1UQ7TnmOiFWA3YDRAJk5C5gVEfsBI6rdxgO3Aif+/Qh/Y9MiSVJNNGt6KCLGAGO6bBqXmeOq5xsBLwEXRcTWwP3AMcDamTkZIDMnR8TQno5j0yJJkpZI1aCMW8jbywLvB47OzHsi4jv0YipoYQNJkqQaaNNC3OeA5zLznur1z2k0LS9GxPAqZRkOTOlpIBfiSpKkpsnMvwJ/iYjNqk17Ao8B1wKHVtsOBa7paSyTFkmSaqKNF5c7Grg0IgYBTwGfoxGcXBERhwHPAgf2NIhNiyRJaqrMfBDYbgFv7bko49i0SJJUE6Vfxt+mRZKkmii9aSm7ekmSVBsmLZIk1YRJiyRJUguYtEiSVBOlJy02LZIk1UTpTUvZ1UuSpNowaZEkqSZMWiRJklrApEWSpJooPWmxaZEkqSZKb1rKrl6SJNWGSYskSTVh0iJJktQCJi2SJNWESYskSVILmLRIklQTpSctNi2SJNVE6U1L2dVLkqTaMGmRJKkmTFokSZJawKRFkqSaKD1psWmRJKkmSm9ayq5ekiTVhkmLJEk1YdIiSZLUAiYtkiTVROlJi02LJEk1UXrTUnb1kiSpNkxaJEmqCZMWSZKkFjBpkSSpJkxaJEmSWsCkRZKkmig9abFpkSSpJkpvWsquXpIk1YZJiyRJNWHSIkmS1AImLZIk1UTpSYtNiyRJNRER7S5hiZTdckmSpNowaZEkqSZKnx4qu3pJklQbJi2SJNVE6WtabFokSaoJp4ckSZJawKRFkqSaKH16yKRFkiQVwaRFkqSacE2LJElSC5i0SJJUE6WvaVmqmpbnnni43SWoH/nlb+5qdwnqZw79yM7tLkFaIk4PSZIktcBSlbRIkqTmKX16yKRFkiQVwaRFkqSaKH1Ni02LJEk14fSQJElSC5i0SJJUE6VPD5VdvSRJqg2TFkmSaqL0NS02LZIk1YTTQ5IkSS1g0iJJUk20a3ooIv4MvAF0AHMyc7uIGAL8FNgQ+DPwqcx8tbtxTFokSVIrjMzMbTJzu+r1WGBCZm4CTKhed8ukRZKkmljK1rTsB4yono8HbgVO7O4DS1X1kiSpPBExJiImdnmMmW+XBG6KiPu7vLd2Zk4GqL4O7ek4Ji2SJNVEs9a0ZOY4YFw3u3woM1+IiKHAzRHxh8U5jk2LJEk10a7pocx8ofo6JSKuArYHXoyI4Zk5OSKGA1N6GsfpIUmS1DQRsWJErDz3OTAKeBS4Fji02u1Q4JqexjJpkSSpJtp0yvPawFXVsZcFLsvMGyLiPuCKiDgMeBY4sKeBbFokSVLTZOZTwNYL2D4V2HNRxrJpkSSpJpayU54XmU2LJEk1UfoNE8tuuSRJUm2YtEiSVBOlTw+VXb0kSaoNkxZJkmqi9DUtNi2SJNWE00OSJEktYNIiSVJNlD49ZNIiSZKKYNIiSVJNmLRIkiS1gEmLJEk1UXrSYtMiSVJNeMqzJElSC5i0SJJUE6VPD5m0SJKkIpi0SJJUE6UnLTYtkiTVROlNi9NDkiSpCCYtkiTVhKc8S5IktYBJiyRJNVH6mhabFkmSaqL0psXpIUmSVASTFkmSaqLfJy0RcUxErBINF0TEAxExqhXFSZIkzdWb6aHPZ+brwChgLeBzwNlNrUqSJPW5iGjKo1V6Mz00t5qPAhdl5kNRer4kSVIN1eE6LfdHxE00mpYbI2JloLO5ZUmSJL1Tb5KWw4BtgKcyc3pErEFjikiSJBWk9ImShTYtEfH++TZtVPo3K0mSytVd0vKtbt5LYI8+rkWSJDVR6eHDQpuWzBzZykIkSZK60+OalohYATgeWD8zx0TEJsBmmXld06uTJEl9pvSkpTdnD10EzAJ2rl4/B5zetIokSVJTlH6dlt40LRtn5jnAbIDMnMHfrt0iSZLUEr055XlWRAymsfiWiNgYeLupVUmSpD5X+sXletO0nALcAKwXEZcCHwJGN7MoSZKk+fXYtGTmzRHxALAjjWmhYzLz5aZXJkmS+lTpC3F7k7QA7A7sQmOKaCBwVdMqkiRJTVF609Lj5FZE/AA4AngEeBT4QkR8v9mFSZIkddWbpGV3YIvMnLsQdzyNBkaSJBWk3yctwBPA+l1erwc83JxyJEmSFqy7Gyb+ksYallWBxyPi3ur1DsDdrSlPkiT1lf58yvO5LatCkiQ1XenTQ93dMPG2VhYiSZLUnd6cPbRjRNwXEW9GxKyI6IiI11tRnCRJ6jul33uoN2cPfQ84CPgZsB3wT8AmzSxK79TR0cEnD/k8Q9dai//6zrm8Nu11jj/p//D8C5NZd53h/MfZ/8aqq6zS7jJViDPPPJPllluOiGDAgAEcc8wx3HTTTdxzzz2suOKKAOyzzz68973vbXOlKsHbb8/is8eMZdbs2XR0dDBq9w/x5c8dzONPPsU3zvsBb8+axYABAzjluC+y1Xs3bXe5KlyvLi6XmX+MiAGZ2QFcFBE9LsSNiAuBfYEpmbnFEtZZaz++/Ao22nBD3nzrLQDOv/gSdvzgBxjzuX9i3EU/5vyLL+Ffvnxkm6tUSY444oh5Dcpcu+66KyNGjGhPQSrWoEEDufi8M1hxhcHMnjOHg48+kd22/wDfvehSjhx9ELvtsB23/c9Evvl/L+KS75zV7nJrr/Q1Lb1ZRjw9IgYBD0bEORFxHLBiTx8CLgb2XpLiBH99cQq33Xk3B+7/8XnbJtx2B/vv+1EA9t/3o/y/W+9oV3mSai4iWHGFwQDMmTOHOXPmzJsyePOtGQC88dZbDF1zSDvLVD/Rm6TlEBrNzVHAcTSu03JATx/KzNsjYsMlqk6c+a1v8y/HHMlbb02ft23q1FcYutaaAAxda01eeeXVdpWnQp1//vkA7Ljjjuy4444A3H333dx///2st9567LvvvqywwgrtLFEF6ejo4BNjjuPZ5yfzmf/9MbZ+32acfNQ/c/hXvs45P7yQzuzk8u99s91lihokLZn5TGbOzMzXM/PUzDweOLOvCoiIMRExMSImjrtwfF8N2y/ccvtdrLH66mzx3s3bXYr6kSOPPJJjjz2Www8/nLvvvpunnnqKnXbaibFjx3Lcccex8sorc91117W7TBVkwIABXH3Bd7n1Zxfx8OOTmPTUM1x+zfWMPfJwbv3ZRZx05OF87ZzvtrtM0bjrcTMerdLbGybOb6e+KiAzxwHjAPLNqdlX4/YHDzz0ML+5/U5uu+u3zJo1izfffIuvfO0brLHGEKa89DJD11qTKS+9zJAhq7e7VBVk1VVXBWCllVZiiy224Nlnn2WjjTaa9/4OO+zAhRde2K7yVLBVVl6J7bfZkjvuvZ+rb/wNXz16DAB7j9iFr33zP9tcnfqDsi+N18+dcPQXue3X1/Cb667kW2eexg4f/ADfPP0b7LHbLlx93fUAXH3d9ey5+65trlSlmDVrFjNnzpz3fNKkSQwbNozXX//bVQweffRRhg0b1q4SVZhXXpvG62+8CcDMt9/mt/c/yEbrv4uhawzh3gcfBeB/HniYDd61TjvL1FzZ2ZxHi3R3Gf/3L+wtYGBzylFv/PPoQzhu7Nf4xTXXMXzY2nz7389od0kqxBtvvMH48Y1p2M7OTrbddls233xzLr/8cl544QUAhgwZwic+8Yl2lqmCvDT1Fcae9W06OjvJzk72HrkLI3fenlVWWpEzvnc+HR0dLDdoEKedcFS7S1U/ENXNm//+jYhbuvtgZo7sduCIy4ERwJrAi8ApmXlBt2M6PaQ+9Mvf3NXuEtTPfPyDri9T34rhm7Z2Zeyc2c35PbvswJZ8H91dxr/bpqQnmfmPS/J5SZLUx7KjSQO3ZgLGNS2SJKkIi3v2kCRJKk0LF802g0mLJEkqQo9JSzQun3cwsFFmnhYR6wPDMvPeplcnSZL6TmfZSUtvpod+AHQCewCnAW8AvwA+2MS6JElSH8smTQ+16hSo3kwP7ZCZRwIzATLzVWBQU6uSJEn9SkQMiIjfRcR11eshEXFzRDxZfe3x8u69aVpmR8QAIKuDrEUjeZEkSSXp7GzOo3eOAR7v8nosMCEzNwEmVK+71Zum5bvAVcDQiDgDuJM+vGGiJEnq3yLiXcDHgB912bwfMPdOyeOB/Xsap8c1LZl5aUTcD+xJY9pq/8x8vIePSZKkpU2TLi4XEWOAMV02jatuiDzXt4F/BVbusm3tzJwMkJmTI2JoT8fpzdlD6wPTgV923ZaZz/b0WUmS1P9VDcq4Bb0XEfsCUzLz/ogYsSTH6c3ZQ7+isZ4lgOWBdwNPAP9rSQ4sSZJarD0Xl/sQ8A8R8VEafcQqEfET4MWIGF6lLMOBKT0N1OOalszcMjO3qr5uAmxPY12LJEkqSRsW4mbmSZn5rszcEDgI+E1mfha4Fji02u1Q4Jqeyl/kK+Jm5gN4jRZJkrRkzgb2iogngb2q193qzZqW47u8XAZ4P/DS4lYoSZLapM33HsrMW4Fbq+dTaZzk02u9WdPSdaXvHBprXH6xKAeRJElaUt02LdVF5VbKzK+0qB5JktQs/fXeQxGxbGbOiYj3t7IgSZLUJE26TkurdJe03Etj/cqDEXEt8DPgrblvZuaVTa5NkiRpnt6saRkCTKVxl+e512tJwKZFkqSStHkh7pLqrmkZWp059Ch/a1bmyqZWJUmSNJ/umpYBwEq8s1mZy6ZFkqTS9NeFuMDkzDytZZVIkqTmKnx6qLsr4i4oYZEkSWqL7pKWRbpKnSRJWsoVPj200KQlM19pZSGSJEnd6c0pz5IkqT8o/OJyi3yXZ0mSpHYwaZEkqSaySWtaWnXmjk2LJEl10Y9PeZYkSVpqmLRIklQXJi2SJEnNZ9IiSVJdFH5xOZsWSZLqwuu0SJIkNZ9JiyRJdVH49JBJiyRJKoJJiyRJdVH4Kc82LZIk1UWnC3ElSZKazqRFkqSaaNYNE1vFpEWSJBXBpEWSpLpwTYskSVLzmbRIklQXhSctNi2SJNVEdpTdtDg9JEmSimDSIklSXXjKsyRJUvOZtEiSVBcuxJUkSSXIwpsWp4ckSVIRTFokSaoLF+JKkiQ1n0mLJEk1UfqaFpsWSZLqovCmxekhSZJUBJMWSZLqwoW4kiRJzWfSIklSTXiXZ0mSpBYwaZEkqS4KP3vIpkWSpLoovGlxekiSJBXBpEWSpJpIT3mWJElqvqUqabl77/3bXYL6kevv+ku7S1A/88SKg9pdgvqZr7w5qbUHLHxNy1LVtEiSpCYqvGlxekiSJBXBpEWSpJpwIa4kSVILmLRIklQXha9psWmRJKkuCm9anB6SJElFMGmRJKkmssOkRZIkqelMWiRJqgtPeZYkSWo+kxZJkuqiDWcPRcTywO3AcjT6jp9n5ikRMQT4KbAh8GfgU5n5andjmbRIklQT2dnRlEcP3gb2yMytgW2AvSNiR2AsMCEzNwEmVK+7ZdMiSZKaJhverF4OrB4J7AeMr7aPB/bvaSynhyRJqolm3XsoIsYAY7psGpeZ47q8PwC4H3gP8P3MvCci1s7MyQCZOTkihvZ0HJsWSZK0RKoGZVw373cA20TEasBVEbHF4hzHpkWSpJrIjvae8pyZr0XErcDewIsRMbxKWYYDU3r6vGtaJEmqiezobMqjOxGxVpWwEBGDgQ8DfwCuBQ6tdjsUuKan+k1aJElSMw0HxlfrWpYBrsjM6yLit8AVEXEY8CxwYE8D2bRIklQTzVqI2+0xMx8Gtl3A9qnAnosyltNDkiSpCCYtkiTVRLsX4i4pmxZJkmqi9KbF6SFJklQEkxZJkmqis6P1N0zsSyYtkiSpCCYtkiTVRDtOee5LJi2SJKkIJi2SJNVE6WcP2bRIklQTpTctTg9JkqQimLRIklQTLsSVJElqAZMWSZJqorPwNS02LZIk1YQLcSVJklrApEWSpJowaZEkSWoBkxZJkmqi9FOebVokSaoJp4ckSZJawKRFkqSaMGmRJElqAZMWSZJqotOFuJIkqQROD0mSJLWASYskSTWRHR3tLmGJmLRIkqQimLRIklQTpV8R16RFkiQVwaRFkqSaKP3sIZsWSZJqovSmxekhSZJUBJMWSZJqotOkRZIkqflMWiRJqonST3m2aZEkqSZciCtJktQCJi2SJNVEdmS7S1giJi2SJKkIJi2SJNVE6ac827RIklQT2en0kCRJUtOZtEiSVBOdLsSVJElqPpMWSZJqwovLSZIktYBJiyRJNVH6xeVsWpZig4YOZdOvfZWBQ4ZAJn+99lom/+znrDFyBOt//vMM3mADHv7nMbz5xBPtLlUFOeSCc9hy3z14Y8pU/m3LjwBwwDknsdXHP8ycWbN4+U/PMv5zX2HGtNfbXKlKsPcPzmSjfUYy/aWpXLz9vgDsfPLRbDX6U8x4+RUAbv/GeTx9023tLFMVF+IuRESsFxG3RMTjEfH7iDimWcfqr7Kjg6e/931+99lDeHjMFxh+wAEM3nBDpj/1NH84+au8/tBD7S5RBfrtxT/nP/c+9B3bHr/5Tk7bYhSnb70PL056mr1P+lKbqlNpHr30Sn6+/2F/t/3+713E+J33Y/zO+9mwqM80M2mZA5yQmQ9ExMrA/RFxc2Y+1sRj9iuzp05l9tSpAHTMmMH0P/+ZQWuuybSJE9tcmUr2xzvuZY0N3vWObY/ffMe850//z+94/yf3aXVZKtRzd01klfXXbXcZ6iUX4i5EZk7OzAeq528AjwP+ZC+m5YYNY6VNN+XNx+z51Fw7f/5AHv31re0uQ4Xb9gufZfT/XMvePziT5VZbpd3lqJ9oydlDEbEhsC1wzwLeGxMREyNi4jV//WsryinOMoMHs/kZp/PUd75Lx/Tp7S5H/dg+Jx9J55wO7r306naXooI9+KPLOH/LD3PxTvvx5osvMfLMse0uSZXOzmzKo1Wa3rRExErAL4BjM/PvVvZl5rjM3C4zt9tv2LBml1OcGDCAzU8/nZduuplXbr+93eWoH9vxnz7BlvvuyQUHu/xMS2b6lKlkZydk8vBFVzBsu63aXZIq2ZFNebRKU5uWiBhIo2G5NDOvbOax+qv3nDSWGc/8mRd++tN2l6J+7H0f2Z2PnHgEP/iHw5k9Y2a7y1HhVlx7rXnPN/n4Xrz82JNtrEb9SdMW4kZEABcAj2fmec06Tn+28lZbMnTvvXnrj39i64suBODZ/xpHDBrIRscey8DVVuO93zyHt578I4+dcEKbq1UpDrvsu2w6YkdWWnN1zvrLb/nlKf/B3id9iWWXG8QxN/8EaCzGveyLX21zpSrBvhedx3q7bs/gNVbniCdu564zvst6u+7A0K02h0ymPfM8N3356+0uU5XOwhfiRmZzYp2I2AW4A3gEmPundHJmXr+wz9y1y65ln0Cupcold/2l3SWon9l4xUHtLkH9zFfenBStPN5v99i9Kb9nd/rNbS35PpqWtGTmnUBL/2NIkqSF84q4kiSpCKU3Ld4wUZIkFcGkRZKkmih9Ia5JiyRJKoJJiyRJNZEtvHptM5i0SJKkpomI9SLiloh4PCJ+HxHHVNuHRMTNEfFk9XX1nsayaZEkqSY6O7Ipjx7MAU7IzPcCOwJHRsT7gLHAhMzcBJhQve6W00OSJNVEtmEhbmZOBiZXz9+IiMeBdYH9gBHVbuOBW4ETuxvLpEWSJC2RiBgTERO7PMYsZL8NgW2Be4C1q4ZmbmMztKfjmLRIklQTzbq4XGaOA8Z1t09ErETjJsrHZubrjVsULhqTFkmS1FQRMZBGw3JpZl5ZbX4xIoZX7w8HpvQ0jkmLJEk10YtFs30uGpHKBcDjmXlel7euBQ4Fzq6+XtPTWDYtkiTVRHa25Yq4HwIOAR6JiAerbSfTaFauiIjDgGeBA3sayKZFkiQ1TWbeCSxsAcueizKWTYskSTXRjumhvuRCXEmSVASTFkmSaqJZpzy3ik2LJEk10Y4r4vYlp4ckSVIRTFokSaoJF+JKkiS1gEmLJEk1UfpCXJMWSZJUBJMWSZJqojPLTlpsWiRJqomOwpsWp4ckSVIRTFokSaqJwtfhmrRIkqQymLRIklQTpa9psWmRJKkmnB6SJElqAZMWSZJqovTpIZMWSZJUBJMWSZJqovQ1LTYtkiTVhNNDkiRJLWDSIklSTZQ+PWTSIkmSimDSIklSTZi0SJIktYBJiyRJNVH62UM2LZIk1YTTQ5IkSS1g0iJJUk2UPj1k0iJJkopg0iJJUk2UvqbFpkWSpJpwekiSJKkFTFokSaqJ0qeHTFokSVIRTFokSaqJ0te02LRIklQTne0uYAk5PSRJkopg0iJJUk2UPj1k0iJJkopg0iJJUk14yrMkSVILmLRIklQTpa9psWmRJKkmnB6SJElqAZMWSZJqovTpIZMWSZJUBJMWSZJqovQ1LTYtkiTVhNNDkiRJLWDSIklSTZQ+PRRZeFRURxExJjPHtbsO9R/+TKmv+TOlZnB6qExj2l2A+h1/ptTX/JlSn7NpkSRJRbBpkSRJRbBpKZPzxOpr/kypr/kzpT7nQlxJklQEkxZJklQEmxZJklQEmxZJklQEm5YCRMRmEbFTRAyMiAHtrkf9hz9P6isR8Z6I2C4ilmt3Leq/XIi7lIuIA4Azgeerx0Tg4sx8va2FqWgRsWlmTqqeD8jMjnbXpHJFxL40/p6aCvwVOGXuz5fUl0xalmIRMRD4NHBYZu4JXAOsB/xrRKzS1uJUrOoXzIMRcRlAZnaYuGhxRcTOwLnAoZk5EngVGNveqtRf2bQs/VYBNqmeXwVcBwwCPhMR0baqVKSIWBE4CjgWmBURPwEbFy2xszPzd9XzU4AhThOpGWxalmKZORs4DzggInbNzE7gTuBBYJd21qYyZeZbwOeBy4B/AZbv2ri0szYV6x7gSpi3Rmo5YAMa/+AiItZoX2nqb2xaln53ADcBh0TEbpnZkZmXAesAW7e3NJUoM1/IzDcz82XgC8DguY1LRLw/IjZvb4UqSfV30tw1dgG8BrySmS9FxMHA6RExuG0Fql9Ztt0FqHuZOTMiLgUSOKn6hfI2sDYwua3FqXiZOTUivgB8MyL+AAwARra5LBUqM+cAb0bEXyLiLGAUMDozZ7S5NPUTNi0FyMxXI+J84DEa/zKeCXw2M19sb2XqDzLz5Yh4GNgH2Cszn2t3TSpTtc5uILBr9XXPzHyyvVWpP/GU58JUc8ZZrW+RllhErA5cAZyQmQ+3ux6VLyJGA/dl5u/bXYv6F5sWSUTE8pk5s911qH+IiEh/uagJbFokSVIRPHtIkiQVwaZFkiQVwaZFkiQVwaZFkiQVwaZFaoGI6IiIByPi0Yj4WUSssARjXRwRn6ye/ygi3tfNviOqG9ot6jH+HBFr9nb7QsYYHRHf64vjShLYtEitMiMzt8nMLYBZwBFd31zcmxVm5uGZ+Vg3u4wAFrlpkaSlkU2L1Hp3AO+pUpBbIuIy4JGIGBAR34yI+yLi4ery+kTD9yLisYj4FTB07kARcWtEbFc93zsiHoiIhyJiQkRsSKM5Oq5KeXaNiLUi4hfVMe6LiA9Vn10jIm6KiN9FxH/RuIdMr0TE9hFxd/XZuyNisy5vrxcRN0TEExFxSpfPfDYi7q3q+q/5m7aIWDEiflV9L49GxKcX9Q9ZUv/jZfylFoqIZWlcLv+GatP2wBaZ+XREjAGmZeYHI2I54K6IuAnYFtgM2JLGPaceAy6cb9y1gPOB3aqxhmTmKxHxf4E3M/Pcar/LgP/IzDsjYn3gRuC9wCnAnZl5WkR8DBizCN/WH6rjzomIDwNnAp/o+v0B04H7qqbrLeDTwIcyc3ZE/AA4GPhxlzH3Bl7IzI9Vda+6CPVI6qdsWqTWGBwRD1bP7wAuoDFtc29mPl1tHwVsNXe9CrAqsAmwG3B5ZnYAL0TEbxYw/o7A7XPHysxXFlLHh4H3NW4RA8AqEbFydYwDqs/+KiJeXYTvbVVgfERsQuPGngO7vHdzZk4FiIgrgV2AOcAHaDQxAIOBKfON+QhwbkT8O3BdZt6xCPVI6qdsWqTWmJGZ23TdUP3CfqvrJuDozLxxvv0+SqMZ6E70Yh9oTAnvNP9dd6taFvfy2P8G3JKZ/7uakrq1y3vzj5lVreMz86SFDZiZkyLiA8BHgbMi4qbMPG0x65PUT7imRVp63Ah8MSIGAkTEphGxInA7cFC15mU4MHIBn/0tsHtEvLv67JBq+xvAyl32uwk4au6LiNimeno7jSkaImIfYPVFqHtV4Pnq+ej53tsrIoZExGBgf+AuYALwyYgYOrfWiNig64ciYh1gemb+BDgXeP8i1COpnzJpkZYePwI2BB6IRvTxEo1f9FcBe9CYMpkE3Db/BzPzpWpNzJURsQyN6Za9gF8CP4+I/YCjgS8D34+Ih2n8/387jcW6pwKXR8QD1fjPdlPnwxEx9y7jVwDn0JgeOh6Yf+rqTuAS4D3AZZk5ESAivgbcVNU6GzgSeKbL57YEvlkdZzbwxW7qkVQT3jBRkiQVwekhSZJUBJsWSZJUBJsWSZJUBJsWSZJUBJsWSZJUBJsWSZJUBJsWSZJUhP8PEXIVjinum9MAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reptile = Reptile(name = 'run1', model = MyPretrainedAlexNet(),dataloaders=meta_learning_loader, test_dataloader=None, tasks=100, n_shot=3, epochs=55, inner_lr=0.0001, meta_lr=0.0001, inner_steps=2, device = device)\n",
    "\n",
    "reptile.test(test_meta_learning_loader,pretrained_model ='REPTILE_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8ae3f5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/asugam/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/user/asugam/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model ...\n",
      "Performing inner loop ...\n",
      " Predicting on query set ...\n",
      "Test loss is 1.086916208267212\n",
      "Accuracy on test task: 0.40555555555555556\n",
      "Confusion Matrix on test task: \n",
      "[[71 57 51]\n",
      " [37 63 33]\n",
      " [19 17 12]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reptile = Reptile(name = 'run1', model = MyPretrainedAlexNet(),dataloaders=meta_learning_loader, test_dataloader=None, tasks=100, n_shot=3, epochs=55, inner_lr=0.0001, meta_lr=0.0001, inner_steps=5, device = device)\n",
    "\n",
    "reptile.test(test_meta_learning_loader,pretrained_model ='REPTILE_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f469c1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/asugam/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/user/asugam/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model ...\n",
      "Performing inner loop ...\n",
      " Predicting on query set ...\n",
      "Test loss is 1.0016645193099976\n",
      "Accuracy on test task: 0.4777777777777778\n",
      "Confusion Matrix on test task: \n",
      "[[57 56 66]\n",
      " [29 87 17]\n",
      " [15  5 28]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reptile = Reptile(name = 'run1', model = MyPretrainedAlexNet(),dataloaders=meta_learning_loader, test_dataloader=None, tasks=100, n_shot=3, epochs=55, inner_lr=0.0001, meta_lr=0.0001, inner_steps=50, device = device)\n",
    "\n",
    "reptile.test(test_meta_learning_loader,pretrained_model ='REPTILE_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "199a5f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/asugam/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/user/asugam/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model ...\n",
      "Performing inner loop ...\n",
      " Predicting on query set ...\n",
      "Test loss is 0.9262685179710388\n",
      "Accuracy on test task: 0.5333333333333333\n",
      "Confusion Matrix on test task: \n",
      "[[ 58  49  72]\n",
      " [ 16 103  14]\n",
      " [ 16   1  31]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reptile = Reptile(name = 'run1', model = MyPretrainedAlexNet(),dataloaders=meta_learning_loader, test_dataloader=None, tasks=100, n_shot=3, epochs=55, inner_lr=0.0001, meta_lr=0.0001, inner_steps=100, device = device)\n",
    "\n",
    "reptile.test(test_meta_learning_loader,pretrained_model ='REPTILE_best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ef8bca1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-1.8782, -1.8782, -1.8782,  ..., -1.8953, -1.8953, -1.9124],\n",
       "           [-1.8782, -1.8610, -1.8610,  ..., -1.8782, -1.8953, -1.8953],\n",
       "           [-1.8782, -1.8610, -1.8782,  ..., -1.8782, -1.8953, -1.9124],\n",
       "           ...,\n",
       "           [-1.8268, -1.8097, -1.7925,  ..., -1.7925, -1.9124, -1.9638],\n",
       "           [-1.8268, -1.8097, -1.8097,  ..., -1.8610, -1.9467, -1.9638],\n",
       "           [-1.8439, -1.8268, -1.8268,  ..., -1.9295, -1.9809, -1.9809]],\n",
       " \n",
       "          [[-1.7906, -1.7906, -1.7906,  ..., -1.8081, -1.8081, -1.8256],\n",
       "           [-1.7906, -1.7731, -1.7731,  ..., -1.7906, -1.8081, -1.8081],\n",
       "           [-1.7906, -1.7731, -1.7906,  ..., -1.7906, -1.8081, -1.8256],\n",
       "           ...,\n",
       "           [-1.7381, -1.7206, -1.7031,  ..., -1.7031, -1.8256, -1.8782],\n",
       "           [-1.7381, -1.7206, -1.7206,  ..., -1.7731, -1.8606, -1.8782],\n",
       "           [-1.7556, -1.7381, -1.7381,  ..., -1.8431, -1.8957, -1.8957]],\n",
       " \n",
       "          [[-1.5604, -1.5604, -1.5604,  ..., -1.5779, -1.5779, -1.5953],\n",
       "           [-1.5604, -1.5430, -1.5430,  ..., -1.5604, -1.5779, -1.5779],\n",
       "           [-1.5604, -1.5430, -1.5604,  ..., -1.5604, -1.5779, -1.5953],\n",
       "           ...,\n",
       "           [-1.5081, -1.4907, -1.4733,  ..., -1.4733, -1.5953, -1.6476],\n",
       "           [-1.5081, -1.4907, -1.4907,  ..., -1.5430, -1.6302, -1.6476],\n",
       "           [-1.5256, -1.5081, -1.5081,  ..., -1.6127, -1.6650, -1.6650]]],\n",
       " \n",
       " \n",
       "         [[[-2.1008, -2.1008, -2.1008,  ..., -2.0837, -2.0837, -2.0837],\n",
       "           [-2.1008, -2.1008, -2.1008,  ..., -2.0665, -2.0665, -2.0665],\n",
       "           [-2.1008, -2.1008, -2.1008,  ..., -2.0665, -2.0665, -2.0665],\n",
       "           ...,\n",
       "           [ 1.2385,  1.4612,  1.4098,  ...,  1.3242,  1.3070,  1.1187],\n",
       "           [ 1.2043,  1.4440,  1.4440,  ...,  1.3242,  1.3242,  1.1358],\n",
       "           [ 0.9303,  1.1529,  1.1872,  ...,  1.1015,  1.1015,  0.9303]],\n",
       " \n",
       "          [[-2.0182, -2.0182, -2.0182,  ..., -2.0007, -2.0007, -2.0007],\n",
       "           [-2.0182, -2.0182, -2.0182,  ..., -1.9832, -1.9832, -1.9832],\n",
       "           [-2.0182, -2.0182, -2.0182,  ..., -1.9832, -1.9832, -1.9832],\n",
       "           ...,\n",
       "           [ 1.3957,  1.6232,  1.5707,  ...,  1.4832,  1.4657,  1.2731],\n",
       "           [ 1.3606,  1.6057,  1.6057,  ...,  1.4832,  1.4832,  1.2906],\n",
       "           [ 1.0805,  1.3081,  1.3431,  ...,  1.2556,  1.2556,  1.0805]],\n",
       " \n",
       "          [[-1.7870, -1.7870, -1.7870,  ..., -1.7696, -1.7696, -1.7696],\n",
       "           [-1.7870, -1.7870, -1.7870,  ..., -1.7522, -1.7522, -1.7522],\n",
       "           [-1.7870, -1.7870, -1.7870,  ..., -1.7522, -1.7522, -1.7522],\n",
       "           ...,\n",
       "           [ 1.6117,  1.8383,  1.7860,  ...,  1.6988,  1.6814,  1.4897],\n",
       "           [ 1.5768,  1.8208,  1.8208,  ...,  1.6988,  1.6988,  1.5071],\n",
       "           [ 1.2980,  1.5245,  1.5594,  ...,  1.4722,  1.4722,  1.2980]]],\n",
       " \n",
       " \n",
       "         [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.0494, -2.0494, -2.0494,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.0323, -2.0323, -2.0323,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [ 0.7591,  1.0844,  1.2043,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [ 0.6049,  0.9474,  1.0844,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [ 0.3138,  0.6392,  0.7933,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-1.9657, -1.9657, -1.9657,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-1.9482, -1.9482, -1.9482,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [ 0.9055,  1.2381,  1.3606,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [ 0.7479,  1.0980,  1.2381,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [ 0.4503,  0.7829,  0.9405,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.7347, -1.7347, -1.7347,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.7173, -1.7173, -1.7173,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [ 1.1237,  1.4548,  1.5768,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [ 0.9668,  1.3154,  1.4548,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [ 0.6705,  1.0017,  1.1585,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 0.4851, -0.1828, -1.0904,  ..., -2.1179, -2.1008, -2.1008],\n",
       "           [-0.2684, -1.1075, -1.7583,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-1.2274, -1.7925, -2.0665,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-0.6281, -1.0562, -1.3815,  ..., -1.6898, -1.8097, -1.9467],\n",
       "           [-0.6281, -1.0048, -1.3130,  ..., -1.5699, -1.7069, -1.8782],\n",
       "           [-0.5767, -0.9192, -1.1932,  ..., -1.4329, -1.6042, -1.8097]],\n",
       " \n",
       "          [[ 0.6254, -0.0574, -0.9853,  ..., -2.0357, -2.0182, -2.0182],\n",
       "           [-0.1450, -1.0028, -1.6681,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-1.1253, -1.7031, -1.9832,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-0.5126, -0.9503, -1.2829,  ..., -1.5980, -1.7206, -1.8606],\n",
       "           [-0.5126, -0.8978, -1.2129,  ..., -1.4755, -1.6155, -1.7906],\n",
       "           [-0.4601, -0.8102, -1.0903,  ..., -1.3354, -1.5105, -1.7206]],\n",
       " \n",
       "          [[ 0.8448,  0.1651, -0.7587,  ..., -1.8044, -1.7870, -1.7870],\n",
       "           [ 0.0779, -0.7761, -1.4384,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-0.8981, -1.4733, -1.7522,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-0.2881, -0.7238, -1.0550,  ..., -1.3687, -1.4907, -1.6302],\n",
       "           [-0.2881, -0.6715, -0.9853,  ..., -1.2467, -1.3861, -1.5604],\n",
       "           [-0.2358, -0.5844, -0.8633,  ..., -1.1073, -1.2816, -1.4907]]],\n",
       " \n",
       " \n",
       "         [[[-2.0152, -2.0152, -2.0152,  ..., -2.0323, -2.0323, -2.0494],\n",
       "           [-2.0152, -2.0152, -2.0152,  ..., -2.0323, -2.0323, -2.0323],\n",
       "           [-2.0152, -2.0152, -2.0152,  ..., -2.0323, -2.0323, -2.0323],\n",
       "           ...,\n",
       "           [ 0.2796,  0.5022,  0.5707,  ..., -2.0837, -2.0837, -2.0837],\n",
       "           [ 0.2796,  0.5022,  0.5707,  ..., -2.0837, -2.0837, -2.0837],\n",
       "           [ 0.1083,  0.3138,  0.3823,  ..., -2.0837, -2.0837, -2.0837]],\n",
       " \n",
       "          [[-1.9307, -1.9307, -1.9307,  ..., -1.9482, -1.9482, -1.9657],\n",
       "           [-1.9307, -1.9307, -1.9307,  ..., -1.9482, -1.9482, -1.9482],\n",
       "           [-1.9307, -1.9307, -1.9307,  ..., -1.9482, -1.9482, -1.9482],\n",
       "           ...,\n",
       "           [ 0.4153,  0.6429,  0.7129,  ..., -2.0007, -2.0007, -2.0007],\n",
       "           [ 0.4153,  0.6429,  0.7129,  ..., -2.0007, -2.0007, -2.0007],\n",
       "           [ 0.2402,  0.4503,  0.5203,  ..., -2.0007, -2.0007, -2.0007]],\n",
       " \n",
       "          [[-1.6999, -1.6999, -1.6999,  ..., -1.7173, -1.7173, -1.7347],\n",
       "           [-1.6999, -1.6999, -1.6999,  ..., -1.7173, -1.7173, -1.7173],\n",
       "           [-1.6999, -1.6999, -1.6999,  ..., -1.7173, -1.7173, -1.7173],\n",
       "           ...,\n",
       "           [ 0.6356,  0.8622,  0.9319,  ..., -1.7696, -1.7696, -1.7696],\n",
       "           [ 0.6356,  0.8622,  0.9319,  ..., -1.7696, -1.7696, -1.7696],\n",
       "           [ 0.4614,  0.6705,  0.7402,  ..., -1.7696, -1.7696, -1.7696]]],\n",
       " \n",
       " \n",
       "         [[[ 0.0398,  0.1939,  0.1939,  ..., -0.9020, -2.1179, -2.1179],\n",
       "           [-0.4054, -0.2856, -0.2513,  ..., -1.1418, -2.1179, -2.1179],\n",
       "           [-0.8335, -0.7479, -0.7137,  ..., -1.4158, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-1.6384, -1.6898, -1.7069,  ..., -1.9467, -2.1179, -2.1179],\n",
       "           [-1.6384, -1.6727, -1.7069,  ..., -1.9467, -2.1179, -2.1179],\n",
       "           [-1.7240, -1.7583, -1.7754,  ..., -1.9809, -2.1179, -2.1179]],\n",
       " \n",
       "          [[ 0.1702,  0.3277,  0.3277,  ..., -0.7927, -2.0357, -2.0357],\n",
       "           [-0.2850, -0.1625, -0.1275,  ..., -1.0378, -2.0357, -2.0357],\n",
       "           [-0.7227, -0.6352, -0.6001,  ..., -1.3179, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-1.5455, -1.5980, -1.6155,  ..., -1.8606, -2.0357, -2.0357],\n",
       "           [-1.5455, -1.5805, -1.6155,  ..., -1.8606, -2.0357, -2.0357],\n",
       "           [-1.6331, -1.6681, -1.6856,  ..., -1.8957, -2.0357, -2.0357]],\n",
       " \n",
       "          [[ 0.3916,  0.5485,  0.5485,  ..., -0.5670, -1.8044, -1.8044],\n",
       "           [-0.0615,  0.0605,  0.0953,  ..., -0.8110, -1.8044, -1.8044],\n",
       "           [-0.4973, -0.4101, -0.3753,  ..., -1.0898, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.3164, -1.3687, -1.3861,  ..., -1.6302, -1.8044, -1.8044],\n",
       "           [-1.3164, -1.3513, -1.3861,  ..., -1.6302, -1.8044, -1.8044],\n",
       "           [-1.4036, -1.4384, -1.4559,  ..., -1.6650, -1.8044, -1.8044]]]]),\n",
       " tensor([1, 0, 2, 1, 2, 0, 0, 2, 1, 1, 0, 0, 2, 1, 2]),\n",
       " tensor([[[[-1.8782, -1.8782, -1.8782,  ..., -1.9638, -1.9638, -1.9809],\n",
       "           [-1.8782, -1.8610, -1.8610,  ..., -1.9467, -1.9467, -1.9638],\n",
       "           [-1.8782, -1.8610, -1.8782,  ..., -1.9467, -1.9467, -1.9638],\n",
       "           ...,\n",
       "           [-0.0116,  0.1254,  0.1254,  ...,  1.7009,  1.7352,  1.5639],\n",
       "           [-0.0116,  0.1254,  0.1254,  ...,  1.7180,  1.7523,  1.5468],\n",
       "           [-0.1657, -0.0287, -0.0287,  ...,  1.4612,  1.4783,  1.2899]],\n",
       " \n",
       "          [[-1.7906, -1.7906, -1.7906,  ..., -1.8782, -1.8782, -1.8957],\n",
       "           [-1.7906, -1.7731, -1.7731,  ..., -1.8606, -1.8606, -1.8782],\n",
       "           [-1.7906, -1.7731, -1.7906,  ..., -1.8606, -1.8606, -1.8782],\n",
       "           ...,\n",
       "           [ 0.1176,  0.2577,  0.2577,  ...,  1.8683,  1.9034,  1.7283],\n",
       "           [ 0.1176,  0.2577,  0.2577,  ...,  1.8859,  1.9209,  1.7108],\n",
       "           [-0.0399,  0.1001,  0.1001,  ...,  1.6232,  1.6408,  1.4482]],\n",
       " \n",
       "          [[-1.5604, -1.5604, -1.5604,  ..., -1.6476, -1.6476, -1.6650],\n",
       "           [-1.5604, -1.5430, -1.5430,  ..., -1.6302, -1.6302, -1.6476],\n",
       "           [-1.5604, -1.5430, -1.5604,  ..., -1.6302, -1.6302, -1.6476],\n",
       "           ...,\n",
       "           [ 0.3393,  0.4788,  0.4788,  ...,  2.0823,  2.1171,  1.9428],\n",
       "           [ 0.3393,  0.4788,  0.4788,  ...,  2.0997,  2.1346,  1.9254],\n",
       "           [ 0.1825,  0.3219,  0.3219,  ...,  1.8383,  1.8557,  1.6640]]],\n",
       " \n",
       " \n",
       "         [[[-0.5082, -1.4158, -1.7754,  ..., -1.8268, -1.8268, -1.8439],\n",
       "           [-0.4739, -1.3987, -1.7583,  ..., -1.8097, -1.8097, -1.8268],\n",
       "           [-0.5424, -1.4329, -1.7754,  ..., -1.8097, -1.8097, -1.8268],\n",
       "           ...,\n",
       "           [ 0.3823,  0.4166,  0.2967,  ..., -1.8268, -1.8097, -1.8097],\n",
       "           [ 0.3481,  0.3652,  0.2282,  ..., -1.8268, -1.8097, -1.8097],\n",
       "           [ 0.1254,  0.1254,  0.0056,  ..., -1.8439, -1.8268, -1.8268]],\n",
       " \n",
       "          [[-0.3901, -1.3179, -1.6856,  ..., -1.7381, -1.7381, -1.7556],\n",
       "           [-0.3550, -1.3004, -1.6681,  ..., -1.7206, -1.7206, -1.7381],\n",
       "           [-0.4251, -1.3354, -1.6856,  ..., -1.7206, -1.7206, -1.7381],\n",
       "           ...,\n",
       "           [ 0.5203,  0.5553,  0.4328,  ..., -1.7381, -1.7206, -1.7206],\n",
       "           [ 0.4853,  0.5028,  0.3627,  ..., -1.7381, -1.7206, -1.7206],\n",
       "           [ 0.2577,  0.2577,  0.1352,  ..., -1.7556, -1.7381, -1.7381]],\n",
       " \n",
       "          [[-0.1661, -1.0898, -1.4559,  ..., -1.5081, -1.5081, -1.5256],\n",
       "           [-0.1312, -1.0724, -1.4384,  ..., -1.4907, -1.4907, -1.5081],\n",
       "           [-0.2010, -1.1073, -1.4559,  ..., -1.4907, -1.4907, -1.5081],\n",
       "           ...,\n",
       "           [ 0.7402,  0.7751,  0.6531,  ..., -1.5081, -1.4907, -1.4907],\n",
       "           [ 0.7054,  0.7228,  0.5834,  ..., -1.5081, -1.4907, -1.4907],\n",
       "           [ 0.4788,  0.4788,  0.3568,  ..., -1.5256, -1.5081, -1.5081]]],\n",
       " \n",
       " \n",
       "         [[[-2.1179, -1.6213, -1.1932,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -1.5870, -1.1247,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -1.5870, -1.1247,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           ...,\n",
       "           [-2.1179, -0.5253,  1.8037,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -0.5082,  1.8208,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.1179, -0.6109,  1.5639,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "          [[-2.0357, -1.5280, -1.0903,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -1.4930, -1.0203,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -1.4930, -1.0203,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           ...,\n",
       "           [-2.0357, -0.4076,  1.9734,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -0.3901,  1.9909,  ..., -2.0357, -2.0357, -2.0357],\n",
       "           [-2.0357, -0.4951,  1.7283,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "          [[-1.8044, -1.2990, -0.8633,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.2641, -0.7936,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -1.2641, -0.7936,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           ...,\n",
       "           [-1.8044, -0.1835,  2.1868,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -0.1661,  2.2043,  ..., -1.8044, -1.8044, -1.8044],\n",
       "           [-1.8044, -0.2707,  1.9428,  ..., -1.8044, -1.8044, -1.8044]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[-2.0494, -1.9980, -1.9809,  ..., -2.1008, -2.1008, -2.1008],\n",
       "           [-2.0494, -1.9980, -1.9809,  ..., -2.1008, -2.1008, -2.1008],\n",
       "           [-2.0152, -1.9638, -1.9295,  ..., -2.1008, -2.1008, -2.1008],\n",
       "           ...,\n",
       "           [-0.5424, -0.0458,  0.0227,  ..., -2.0837, -2.0837, -2.0837],\n",
       "           [-0.5596, -0.0287,  0.0569,  ..., -2.0837, -2.0837, -2.0837],\n",
       "           [-0.8507, -0.3712, -0.3198,  ..., -2.0837, -2.0837, -2.0837]],\n",
       " \n",
       "          [[-1.9657, -1.9132, -1.8957,  ..., -2.0182, -2.0182, -2.0182],\n",
       "           [-1.9657, -1.9132, -1.8957,  ..., -2.0182, -2.0182, -2.0182],\n",
       "           [-1.9307, -1.8782, -1.8431,  ..., -2.0182, -2.0182, -2.0182],\n",
       "           ...,\n",
       "           [-0.4251,  0.0826,  0.1527,  ..., -2.0007, -2.0007, -2.0007],\n",
       "           [-0.4426,  0.1001,  0.1877,  ..., -2.0007, -2.0007, -2.0007],\n",
       "           [-0.7402, -0.2500, -0.1975,  ..., -2.0007, -2.0007, -2.0007]],\n",
       " \n",
       "          [[-1.7347, -1.6824, -1.6650,  ..., -1.7870, -1.7870, -1.7870],\n",
       "           [-1.7347, -1.6824, -1.6650,  ..., -1.7870, -1.7870, -1.7870],\n",
       "           [-1.6999, -1.6476, -1.6127,  ..., -1.7870, -1.7870, -1.7870],\n",
       "           ...,\n",
       "           [-0.2010,  0.3045,  0.3742,  ..., -1.7696, -1.7696, -1.7696],\n",
       "           [-0.2184,  0.3219,  0.4091,  ..., -1.7696, -1.7696, -1.7696],\n",
       "           [-0.5147, -0.0267,  0.0256,  ..., -1.7696, -1.7696, -1.7696]]],\n",
       " \n",
       " \n",
       "         [[[-2.1008, -2.1008, -2.1008,  ..., -2.0665, -2.0665, -2.0665],\n",
       "           [-2.1008, -2.1008, -2.1008,  ..., -2.0837, -2.0837, -2.0665],\n",
       "           [-2.1008, -2.1008, -2.1008,  ..., -2.0837, -2.0837, -2.0837],\n",
       "           ...,\n",
       "           [-1.6727, -1.3644, -1.0904,  ..., -1.9809, -1.9467, -1.9295],\n",
       "           [-1.8782, -1.7925, -1.5528,  ..., -1.9809, -1.9638, -1.9295],\n",
       "           [-1.9467, -1.7583, -1.4843,  ..., -1.9980, -1.9809, -1.9638]],\n",
       " \n",
       "          [[-2.0182, -2.0182, -2.0182,  ..., -1.9832, -1.9832, -1.9832],\n",
       "           [-2.0182, -2.0182, -2.0182,  ..., -2.0007, -2.0007, -1.9832],\n",
       "           [-2.0182, -2.0182, -2.0182,  ..., -2.0007, -2.0007, -2.0007],\n",
       "           ...,\n",
       "           [-1.5805, -1.2654, -0.9853,  ..., -1.8957, -1.8606, -1.8431],\n",
       "           [-1.7906, -1.7031, -1.4580,  ..., -1.8957, -1.8782, -1.8431],\n",
       "           [-1.8606, -1.6681, -1.3880,  ..., -1.9132, -1.8957, -1.8782]],\n",
       " \n",
       "          [[-1.7870, -1.7870, -1.7870,  ..., -1.7522, -1.7522, -1.7522],\n",
       "           [-1.7870, -1.7870, -1.7870,  ..., -1.7696, -1.7696, -1.7522],\n",
       "           [-1.7870, -1.7870, -1.7870,  ..., -1.7696, -1.7696, -1.7696],\n",
       "           ...,\n",
       "           [-1.3513, -1.0376, -0.7587,  ..., -1.6650, -1.6302, -1.6127],\n",
       "           [-1.5604, -1.4733, -1.2293,  ..., -1.6650, -1.6476, -1.6127],\n",
       "           [-1.6302, -1.4384, -1.1596,  ..., -1.6824, -1.6650, -1.6476]]],\n",
       " \n",
       " \n",
       "         [[[-1.9638, -1.8953, -1.8782,  ..., -2.0323, -2.0323, -2.0323],\n",
       "           [-1.8953, -1.7754, -1.7583,  ..., -2.0323, -2.0323, -2.0323],\n",
       "           [-1.8097, -1.6727, -1.6555,  ..., -2.0323, -2.0323, -2.0323],\n",
       "           ...,\n",
       "           [-0.0116,  0.8618,  0.9132,  ..., -0.4739, -0.1486, -0.1486],\n",
       "           [-0.0287,  0.8447,  0.9303,  ..., -0.4226, -0.1314, -0.1486],\n",
       "           [-0.4054,  0.3138,  0.3652,  ..., -0.7479, -0.5596, -0.5596]],\n",
       " \n",
       "          [[-1.8782, -1.8081, -1.7906,  ..., -1.9482, -1.9482, -1.9482],\n",
       "           [-1.8081, -1.6856, -1.6681,  ..., -1.9482, -1.9482, -1.9482],\n",
       "           [-1.7206, -1.5805, -1.5630,  ..., -1.9482, -1.9482, -1.9482],\n",
       "           ...,\n",
       "           [ 0.1176,  1.0105,  1.0630,  ..., -0.3550, -0.0224, -0.0224],\n",
       "           [ 0.1001,  0.9930,  1.0805,  ..., -0.3025, -0.0049, -0.0224],\n",
       "           [-0.2850,  0.4503,  0.5028,  ..., -0.6352, -0.4426, -0.4426]],\n",
       " \n",
       "          [[-1.6476, -1.5779, -1.5604,  ..., -1.7173, -1.7173, -1.7173],\n",
       "           [-1.5779, -1.4559, -1.4384,  ..., -1.7173, -1.7173, -1.7173],\n",
       "           [-1.4907, -1.3513, -1.3339,  ..., -1.7173, -1.7173, -1.7173],\n",
       "           ...,\n",
       "           [ 0.3393,  1.2282,  1.2805,  ..., -0.1312,  0.1999,  0.1999],\n",
       "           [ 0.3219,  1.2108,  1.2980,  ..., -0.0790,  0.2173,  0.1999],\n",
       "           [-0.0615,  0.6705,  0.7228,  ..., -0.4101, -0.2184, -0.2184]]]]),\n",
       " tensor([1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 1, 0, 2, 2, 1, 0, 0, 1,\n",
       "         0, 0, 2, 1, 0, 0, 2, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 2, 1, 0, 0,\n",
       "         2, 2, 0, 2, 0, 1, 1, 2, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 2, 1, 1, 2, 1,\n",
       "         1, 0, 1, 0, 0, 1, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 1, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2,\n",
       "         0, 1, 0, 1, 2, 0, 1, 1, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 1, 1,\n",
       "         0, 2, 1, 1, 0, 0, 2, 1, 0, 0, 2, 2, 0, 2, 1, 0, 0, 0, 2, 0, 0, 1, 1, 0,\n",
       "         1, 0, 0, 0, 1, 1, 2, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 2, 0, 1,\n",
       "         2, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 2, 2, 1, 0, 1, 2, 1, 1, 1, 0, 0, 0, 0, 2, 2, 1, 2, 1, 1,\n",
       "         0, 1, 1, 2, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0,\n",
       "         0, 1, 1, 0, 0, 0, 1, 0, 1, 2, 2, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_meta_learning_loader[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b93cb9",
   "metadata": {},
   "source": [
    "# REPTILE WITH SCHEDULER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3adfb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReptileCloneScheduled:\n",
    "    def __init__(self, name, model,dataloaders, test_dataloader=None, tasks=100, n_shot=3, epochs=50, inner_lr=0.01, meta_lr=0.001, inner_steps=10, device = 'cuda'):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        #print(self.model)\n",
    "        #print(self.device)\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.name = name\n",
    "        self.tasks = tasks\n",
    "        self.n_shot = n_shot\n",
    "        self.epochs = epochs\n",
    "        self.dataloaders = dataloaders\n",
    "        #self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr_outer)\n",
    "        self.test_dataloader = test_dataloader\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.inner_optimizer = optim.SGD(self.model.parameters(), lr=self.inner_lr)\n",
    "\n",
    "    def inner_loop(self, task_support_set):\n",
    "\n",
    "        X_train, y_train = task_support_set\n",
    "\n",
    "        for t_epoch in range(self.inner_steps):\n",
    "            self.model.train()\n",
    "            preds = self.model(X_train)\n",
    "            loss = nn.CrossEntropyLoss()(preds, y_train)\n",
    "            self.inner_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.inner_optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "    def outer_loop(self, meta_lr_weight):\n",
    "\n",
    "        meta_weights = {name: param.clone() for name, param in self.model.named_parameters() if 'running_mean' not in name and 'running_var' not in name}\n",
    "        task_losses = 0.0\n",
    "        update_directions = {name: 0 for name, _ in meta_weights.items()}\n",
    "        \n",
    "        for task in range(self.tasks):\n",
    "            if task%1==0:\n",
    "                print(\"\\nWorking with task: {}\".format(task + 1))\n",
    "            support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch = self.dataloaders[task]\n",
    "\n",
    "            if len(query_set_X_batch.shape) == 5:\n",
    "              support_set_X_batch = support_set_X_batch.squeeze(0)\n",
    "\n",
    "            if len(query_set_X_batch.shape) == 5:\n",
    "              query_set_X_batch = query_set_X_batch.squeeze(0)\n",
    "\n",
    "            support_set_X_batch =support_set_X_batch.to(self.device)\n",
    "            support_set_y_batch = support_set_y_batch.to(self.device)\n",
    "            query_set_X_batch = query_set_X_batch.to(self.device)\n",
    "            query_set_y_batch = query_set_y_batch.to(self.device)\n",
    "            #print(\"shapes support set \", support_set_X_batch.shape, support_set_y_batch.shape, sep = ' : ')\n",
    "            task_losses += self.inner_loop((support_set_X_batch, support_set_y_batch))\n",
    "\n",
    "            for name, param in self.model.named_parameters():\n",
    "              if 'running_mean' not in name and 'running_var' not in name:\n",
    "                    update_directions[name] += (param.detach() - meta_weights[name])\n",
    "        \n",
    "        update_directions = {name: direction / self.tasks for name, direction in update_directions.items()}\n",
    "        # Apply meta update\n",
    "        with torch.no_grad():\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if 'running_mean' not in name and 'running_var' not in name:\n",
    "                    param += meta_lr_weight * self.meta_lr * update_directions[name]\n",
    "                    \n",
    "        \n",
    "        self.model.train()\n",
    "        self.model.load_state_dict(meta_weights)\n",
    "        return task_losses/self.tasks\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        print('Start training...')\n",
    "        best_loss = float('inf')\n",
    "        best_model_weights = None\n",
    "        meta_losses = []\n",
    "        try:\n",
    "          for epoch in tqdm(range(self.epochs)):\n",
    "              meta_lr_weight = (1 - (epoch+1)/self.epochs)\n",
    "              meta_loss = self.outer_loop(meta_lr_weight)\n",
    "              meta_losses.append(meta_loss)\n",
    "              if epoch % 1 == 0:\n",
    "                  print('Epoch: %d, Meta Loss: ', epoch, meta_loss)\n",
    "              if meta_loss < best_loss:\n",
    "                  best_loss = meta_loss\n",
    "                  best_model_weights = self.model.state_dict()\n",
    "\n",
    "          torch.save(best_model_weights, '{self.name}_best_model.pth')\n",
    "          print('Training finished.')\n",
    "        except KeyboardInterrupt:\n",
    "          torch.save(self.model.state_dict(), '{self.name}REPTILE_latest_model_interrupted_loss{meta_loss:.3f}.pth')\n",
    "          if best_model_weights:\n",
    "            torch.save(best_model_weights, '{self.name}REPTILE_best_model.pth')\n",
    "        finally:\n",
    "          plt.plot(meta_losses)\n",
    "          plt.xlabel('Epoch')\n",
    "          plt.ylabel('Loss')\n",
    "          plt.title('Meta Training Loss')\n",
    "          plt.show()\n",
    "\n",
    "    def test(self, test_dataloader, pretrained_model = None):\n",
    "\n",
    "      if pretrained_model:\n",
    "        state_dict = torch.load(pretrained_model)\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "\n",
    "      fast_weights = deepcopy(self.model.state_dict())\n",
    "      for task in range(len(test_dataloader)):\n",
    "\n",
    "          support_set_X_batch, support_set_y_batch, query_set_X_batch, query_set_y_batch = test_dataloader[task]\n",
    "          if len(query_set_X_batch.shape) == 5:\n",
    "              support_set_X_batch = support_set_X_batch.squeeze(0)\n",
    "\n",
    "          if len(query_set_X_batch.shape) == 5:\n",
    "            query_set_X_batch = query_set_X_batch.squeeze(0)\n",
    "\n",
    "          support_set_X_batch =support_set_X_batch.to(self.device)\n",
    "          support_set_y_batch = support_set_y_batch.to(self.device)\n",
    "          query_set_X_batch = query_set_X_batch.to(self.device)\n",
    "          query_set_y_batch = query_set_y_batch.to(self.device)\n",
    "\n",
    "          model_copy = self.inner_loop((support_set_X_batch, support_set_y_batch))\n",
    "\n",
    "          model_copy.eval()\n",
    "          with torch.no_grad():\n",
    "              preds = model_copy(query_set_X_batch)\n",
    "              _, preds = torch.max(preds, dim=1)\n",
    "              accuracy = accuracy_score(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "              print(f\"Accuracy on test task: {accuracy}\")\n",
    "              conf_mat = confusion_matrix(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "              print(f'Confusion Matrix on test task: \\n{conf_mat}')\n",
    "              confusion_mat(query_set_y_batch.cpu().detach().numpy(), preds.cpu().detach().numpy())\n",
    "\n",
    "      self.model.train()  # Set the model back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090cbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user/asugam/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/user/asugam/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      "Working with task: 1\n",
      "\n",
      "Working with task: 2\n",
      "\n",
      "Working with task: 3\n",
      "\n",
      "Working with task: 4\n",
      "\n",
      "Working with task: 5\n",
      "\n",
      "Working with task: 6\n",
      "\n",
      "Working with task: 7\n",
      "\n",
      "Working with task: 8\n",
      "\n",
      "Working with task: 9\n",
      "\n",
      "Working with task: 10\n",
      "\n",
      "Working with task: 11\n",
      "\n",
      "Working with task: 12\n",
      "\n",
      "Working with task: 13\n",
      "\n",
      "Working with task: 14\n",
      "\n",
      "Working with task: 15\n",
      "\n",
      "Working with task: 16\n",
      "\n",
      "Working with task: 17\n",
      "\n",
      "Working with task: 18\n",
      "\n",
      "Working with task: 19\n",
      "\n",
      "Working with task: 20\n",
      "\n",
      "Working with task: 21\n",
      "\n",
      "Working with task: 22\n",
      "\n",
      "Working with task: 23\n",
      "\n",
      "Working with task: 24\n",
      "\n",
      "Working with task: 25\n",
      "\n",
      "Working with task: 26\n",
      "\n",
      "Working with task: 27\n",
      "\n",
      "Working with task: 28\n",
      "\n",
      "Working with task: 29\n",
      "\n",
      "Working with task: 30\n",
      "\n",
      "Working with task: 31\n",
      "\n",
      "Working with task: 32\n",
      "\n",
      "Working with task: 33\n",
      "\n",
      "Working with task: 34\n",
      "\n",
      "Working with task: 35\n",
      "\n",
      "Working with task: 36\n",
      "\n",
      "Working with task: 37\n",
      "\n",
      "Working with task: 38\n",
      "\n",
      "Working with task: 39\n",
      "\n",
      "Working with task: 40\n",
      "\n",
      "Working with task: 41\n",
      "\n",
      "Working with task: 42\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "reptile = ReptileCloneScheduled(name = 'run1', model = MyPretrainedAlexNet(),dataloaders=meta_learning_loader, test_dataloader=None, tasks=100, n_shot=3, epochs=55, inner_lr=0.0001, meta_lr=0.01, inner_steps=2, device = device)\n",
    "reptile.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a54e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f912712e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffee195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f89db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a68fa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d4e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a596d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b749776f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513cc299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8df03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da8a276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
